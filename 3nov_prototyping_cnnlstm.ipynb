{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50413aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant ID: 14\n",
      "Projects for ashlin: ['ashlin_phase1', 'ashlin_phase2']\n",
      "Processing project: ashlin_phase1\n",
      "Processing project: ashlin_phase2\n",
      "Splitting data across windows for cross-validation.\n",
      "torch.Size([5067, 6, 3000]) torch.Size([5067, 1])\n",
      "torch.Size([3379, 6, 3000]) torch.Size([3379, 1])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.utils import (\n",
    "    load_config,\n",
    "    get_experiment_dir,\n",
    "    make_windowed_dataset_from_sessions,\n",
    "    get_participant_id,\n",
    "    get_participant_projects,\n",
    "    get_raw_dataset_path,\n",
    "    get_sessions_for_project,\n",
    "    generate_dataset_summary,\n",
    "    load_data_for_participant\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.metrics import f1_score\n",
    "from lib.models import ObnoxiouslySimpleCNN\n",
    "\n",
    "# ejaz\n",
    "# asfik\n",
    "# alsaad\n",
    "# anam\n",
    "# iftakhar\n",
    "# mariah, tj, stephanie, ashlin\n",
    "#  might be fine with window split\n",
    "participant = 'ejaz'\n",
    "labeling = f'andrew smoking labels'\n",
    "window_size = 3000\n",
    "window_stride = 3000\n",
    "sensor_config = {'use_accelerometer': True, 'use_gyroscope': True}\n",
    "model = 'ObnoxiouslySimpleCNN'\n",
    "\n",
    "X_train, y_train, X_val, y_val, train_sessions, val_sessions = load_data_for_participant(participant, window_size, window_stride, labeling, sensor_config, split_across_windows=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "281f1767",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObnoxiouslySimpleCNN(input_channels=6, channels=[64,64,64,64,128], kernel_sizes=[7,3,3,3,3], dilations=[1,2,4,8,16], dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5)\n",
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3a5071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.1684, val loss 0.1130, val f1 0.4934, lr 0.000300\n",
      "Epoch 1: train loss 0.1134, val loss 0.1124, val f1 0.4934, lr 0.000300\n",
      "Epoch 2: train loss 0.1128, val loss 0.1027, val f1 0.4934, lr 0.000300\n",
      "Epoch 3: train loss 0.1064, val loss 0.1058, val f1 0.4934, lr 0.000300\n",
      "Epoch 4: train loss 0.1056, val loss 0.1029, val f1 0.4934, lr 0.000300\n",
      "Epoch 5: train loss 0.1064, val loss 0.1029, val f1 0.4934, lr 0.000300\n",
      "Epoch 6: train loss 0.1041, val loss 0.1034, val f1 0.4934, lr 0.000300\n",
      "Epoch 7: train loss 0.1034, val loss 0.0986, val f1 0.4934, lr 0.000300\n",
      "Epoch 8: train loss 0.1013, val loss 0.1004, val f1 0.4934, lr 0.000300\n",
      "Epoch 9: train loss 0.0998, val loss 0.1061, val f1 0.4934, lr 0.000300\n",
      "Epoch 10: train loss 0.0984, val loss 0.0971, val f1 0.5158, lr 0.000300\n",
      "Epoch 11: train loss 0.1013, val loss 0.0970, val f1 0.5569, lr 0.000300\n",
      "Epoch 12: train loss 0.0978, val loss 0.0954, val f1 0.5569, lr 0.000300\n",
      "Epoch 13: train loss 0.0960, val loss 0.0989, val f1 0.5475, lr 0.000300\n",
      "Epoch 14: train loss 0.0965, val loss 0.0941, val f1 0.5351, lr 0.000300\n",
      "Epoch 15: train loss 0.0947, val loss 0.0970, val f1 0.5432, lr 0.000300\n",
      "Epoch 16: train loss 0.0930, val loss 0.0933, val f1 0.5475, lr 0.000300\n",
      "Epoch 17: train loss 0.0953, val loss 0.0977, val f1 0.5444, lr 0.000300\n",
      "Epoch 18: train loss 0.0918, val loss 0.0948, val f1 0.5527, lr 0.000300\n",
      "Epoch 19: train loss 0.0921, val loss 0.0920, val f1 0.5555, lr 0.000300\n",
      "Epoch 20: train loss 0.0899, val loss 0.0929, val f1 0.5444, lr 0.000300\n",
      "Epoch 21: train loss 0.0913, val loss 0.0952, val f1 0.5728, lr 0.000300\n",
      "Epoch 22: train loss 0.0888, val loss 0.0942, val f1 0.5829, lr 0.000300\n",
      "Epoch 23: train loss 0.0909, val loss 0.0932, val f1 0.5555, lr 0.000300\n",
      "Epoch 24: train loss 0.0925, val loss 0.0938, val f1 0.5158, lr 0.000300\n",
      "Epoch 25: train loss 0.0926, val loss 0.0920, val f1 0.5929, lr 0.000300\n",
      "Epoch 26: train loss 0.0898, val loss 0.0980, val f1 0.5676, lr 0.000300\n",
      "Epoch 27: train loss 0.0913, val loss 0.0935, val f1 0.6141, lr 0.000300\n",
      "Epoch 28: train loss 0.0890, val loss 0.0910, val f1 0.6205, lr 0.000300\n",
      "Epoch 29: train loss 0.0876, val loss 0.0929, val f1 0.6018, lr 0.000300\n",
      "Epoch 30: train loss 0.0869, val loss 0.0886, val f1 0.6070, lr 0.000300\n",
      "Epoch 31: train loss 0.0890, val loss 0.0896, val f1 0.6037, lr 0.000300\n",
      "Epoch 32: train loss 0.0865, val loss 0.0910, val f1 0.6107, lr 0.000300\n",
      "Epoch 33: train loss 0.0875, val loss 0.0899, val f1 0.6107, lr 0.000300\n",
      "Epoch 34: train loss 0.0869, val loss 0.0877, val f1 0.6119, lr 0.000300\n",
      "Epoch 35: train loss 0.0860, val loss 0.0953, val f1 0.5764, lr 0.000300\n",
      "Epoch 36: train loss 0.0871, val loss 0.0907, val f1 0.6007, lr 0.000300\n",
      "Epoch 37: train loss 0.0841, val loss 0.0972, val f1 0.6255, lr 0.000300\n",
      "Epoch 38: train loss 0.0865, val loss 0.0911, val f1 0.5838, lr 0.000300\n",
      "Epoch 39: train loss 0.0849, val loss 0.0888, val f1 0.6048, lr 0.000300\n",
      "Epoch 40: train loss 0.0848, val loss 0.0902, val f1 0.5975, lr 0.000300\n",
      "Epoch 41: train loss 0.0833, val loss 0.0869, val f1 0.6007, lr 0.000300\n",
      "Epoch 42: train loss 0.0841, val loss 0.0900, val f1 0.6349, lr 0.000300\n",
      "Epoch 43: train loss 0.0834, val loss 0.0889, val f1 0.6278, lr 0.000300\n",
      "Epoch 44: train loss 0.0828, val loss 0.0900, val f1 0.6211, lr 0.000300\n",
      "Epoch 45: train loss 0.0835, val loss 0.0873, val f1 0.6107, lr 0.000300\n",
      "Epoch 46: train loss 0.0819, val loss 0.0931, val f1 0.6074, lr 0.000300\n",
      "Epoch 47: train loss 0.0796, val loss 0.0900, val f1 0.6179, lr 0.000300\n",
      "Epoch 48: train loss 0.0789, val loss 0.0854, val f1 0.6459, lr 0.000300\n",
      "Epoch 49: train loss 0.0773, val loss 0.0872, val f1 0.6130, lr 0.000300\n",
      "Epoch 50: train loss 0.0793, val loss 0.0912, val f1 0.6431, lr 0.000300\n",
      "Epoch 51: train loss 0.0813, val loss 0.0866, val f1 0.6163, lr 0.000300\n",
      "Epoch 52: train loss 0.0740, val loss 0.0871, val f1 0.6459, lr 0.000300\n",
      "Epoch 53: train loss 0.0779, val loss 0.0947, val f1 0.6265, lr 0.000300\n",
      "Epoch 54: train loss 0.0779, val loss 0.0861, val f1 0.6368, lr 0.000300\n",
      "Epoch 55: train loss 0.0744, val loss 0.0874, val f1 0.6179, lr 0.000300\n",
      "Epoch 56: train loss 0.0771, val loss 0.0950, val f1 0.6094, lr 0.000300\n",
      "Epoch 57: train loss 0.0731, val loss 0.0876, val f1 0.6391, lr 0.000300\n",
      "Epoch 58: train loss 0.0698, val loss 0.0866, val f1 0.6506, lr 0.000300\n",
      "Epoch 59: train loss 0.0733, val loss 0.0894, val f1 0.6365, lr 0.000300\n",
      "Epoch 60: train loss 0.0718, val loss 0.0898, val f1 0.6550, lr 0.000300\n",
      "Epoch 61: train loss 0.0697, val loss 0.0925, val f1 0.6166, lr 0.000300\n",
      "Epoch 62: train loss 0.0699, val loss 0.0898, val f1 0.6621, lr 0.000300\n",
      "Epoch 63: train loss 0.0705, val loss 0.0902, val f1 0.6342, lr 0.000300\n",
      "Epoch 64: train loss 0.0717, val loss 0.0896, val f1 0.6289, lr 0.000300\n",
      "Epoch 65: train loss 0.0705, val loss 0.0903, val f1 0.6223, lr 0.000300\n",
      "Epoch 66: train loss 0.0669, val loss 0.0914, val f1 0.6119, lr 0.000300\n",
      "Epoch 67: train loss 0.0645, val loss 0.0918, val f1 0.6344, lr 0.000300\n",
      "Epoch 68: train loss 0.0682, val loss 0.0909, val f1 0.6106, lr 0.000300\n",
      "Epoch 69: train loss 0.0660, val loss 0.0948, val f1 0.6485, lr 0.000300\n",
      "Epoch 70: train loss 0.0645, val loss 0.0903, val f1 0.6450, lr 0.000300\n",
      "Epoch 71: train loss 0.0620, val loss 0.0984, val f1 0.6291, lr 0.000300\n",
      "Epoch 72: train loss 0.0625, val loss 0.0869, val f1 0.6520, lr 0.000300\n",
      "Epoch 73: train loss 0.0623, val loss 0.0968, val f1 0.6291, lr 0.000300\n",
      "Epoch 74: train loss 0.0593, val loss 0.0903, val f1 0.6550, lr 0.000300\n",
      "Epoch 75: train loss 0.0608, val loss 0.0975, val f1 0.6508, lr 0.000300\n",
      "Epoch 76: train loss 0.0587, val loss 0.0989, val f1 0.6205, lr 0.000300\n",
      "Epoch 77: train loss 0.0578, val loss 0.0935, val f1 0.6291, lr 0.000300\n",
      "Epoch 78: train loss 0.0554, val loss 0.1054, val f1 0.6397, lr 0.000300\n",
      "Epoch 79: train loss 0.0542, val loss 0.0963, val f1 0.6330, lr 0.000300\n",
      "Epoch 80: train loss 0.0547, val loss 0.1036, val f1 0.6392, lr 0.000300\n",
      "Epoch 81: train loss 0.0553, val loss 0.0956, val f1 0.6592, lr 0.000300\n",
      "Epoch 82: train loss 0.0544, val loss 0.0960, val f1 0.6291, lr 0.000300\n",
      "Epoch 83: train loss 0.0488, val loss 0.1069, val f1 0.6396, lr 0.000300\n",
      "Epoch 84: train loss 0.0489, val loss 0.1052, val f1 0.6459, lr 0.000300\n",
      "Epoch 85: train loss 0.0527, val loss 0.1032, val f1 0.6211, lr 0.000300\n",
      "Epoch 86: train loss 0.0483, val loss 0.0981, val f1 0.6550, lr 0.000300\n",
      "Epoch 87: train loss 0.0425, val loss 0.1179, val f1 0.6249, lr 0.000300\n",
      "Epoch 88: train loss 0.0472, val loss 0.1013, val f1 0.7030, lr 0.000300\n",
      "Epoch 89: train loss 0.0448, val loss 0.1038, val f1 0.6536, lr 0.000300\n",
      "Epoch 90: train loss 0.0443, val loss 0.1235, val f1 0.6368, lr 0.000300\n",
      "Epoch 91: train loss 0.0453, val loss 0.1056, val f1 0.6445, lr 0.000300\n",
      "Epoch 92: train loss 0.0443, val loss 0.1047, val f1 0.6536, lr 0.000300\n",
      "Epoch 93: train loss 0.0443, val loss 0.1053, val f1 0.6863, lr 0.000300\n",
      "Epoch 94: train loss 0.0406, val loss 0.1167, val f1 0.6191, lr 0.000300\n",
      "Epoch 95: train loss 0.0372, val loss 0.1145, val f1 0.6510, lr 0.000300\n",
      "Epoch 96: train loss 0.0416, val loss 0.1207, val f1 0.6163, lr 0.000300\n",
      "Epoch 97: train loss 0.0394, val loss 0.1135, val f1 0.6632, lr 0.000300\n",
      "Epoch 98: train loss 0.0414, val loss 0.1086, val f1 0.6579, lr 0.000300\n",
      "Epoch 99: train loss 0.0362, val loss 0.1227, val f1 0.6278, lr 0.000300\n",
      "Epoch 100: train loss 0.0359, val loss 0.1115, val f1 0.6574, lr 0.000300\n",
      "Epoch 101: train loss 0.0325, val loss 0.1225, val f1 0.6330, lr 0.000300\n",
      "Epoch 102: train loss 0.0322, val loss 0.1196, val f1 0.6417, lr 0.000300\n",
      "Epoch 103: train loss 0.0336, val loss 0.1181, val f1 0.6563, lr 0.000300\n",
      "Epoch 104: train loss 0.0349, val loss 0.1204, val f1 0.6490, lr 0.000300\n",
      "Epoch 105: train loss 0.0275, val loss 0.1286, val f1 0.6352, lr 0.000300\n",
      "Epoch 106: train loss 0.0300, val loss 0.1200, val f1 0.6893, lr 0.000300\n",
      "Epoch 107: train loss 0.0319, val loss 0.1213, val f1 0.6450, lr 0.000300\n",
      "Epoch 108: train loss 0.0386, val loss 0.1181, val f1 0.6265, lr 0.000300\n",
      "Epoch 109: train loss 0.0298, val loss 0.1301, val f1 0.6550, lr 0.000300\n",
      "Epoch 110: train loss 0.0280, val loss 0.1202, val f1 0.6597, lr 0.000300\n",
      "Epoch 111: train loss 0.0247, val loss 0.1326, val f1 0.6253, lr 0.000300\n",
      "Epoch 112: train loss 0.0221, val loss 0.1318, val f1 0.6821, lr 0.000300\n",
      "Epoch 113: train loss 0.0267, val loss 0.1422, val f1 0.6755, lr 0.000300\n",
      "Epoch 114: train loss 0.0268, val loss 0.1250, val f1 0.6878, lr 0.000300\n",
      "Epoch 115: train loss 0.0299, val loss 0.1322, val f1 0.6500, lr 0.000300\n",
      "Epoch 116: train loss 0.0307, val loss 0.1212, val f1 0.6788, lr 0.000300\n",
      "Epoch 117: train loss 0.0254, val loss 0.1432, val f1 0.6370, lr 0.000300\n",
      "Epoch 118: train loss 0.0336, val loss 0.1196, val f1 0.6496, lr 0.000300\n",
      "Epoch 119: train loss 0.0277, val loss 0.1385, val f1 0.6492, lr 0.000300\n",
      "Epoch 120: train loss 0.0181, val loss 0.1373, val f1 0.6671, lr 0.000300\n",
      "Epoch 121: train loss 0.0248, val loss 0.1499, val f1 0.6142, lr 0.000300\n",
      "Epoch 122: train loss 0.0251, val loss 0.1308, val f1 0.6522, lr 0.000300\n",
      "Epoch 123: train loss 0.0195, val loss 0.1357, val f1 0.6698, lr 0.000300\n",
      "Epoch 124: train loss 0.0197, val loss 0.1442, val f1 0.6627, lr 0.000300\n",
      "Epoch 125: train loss 0.0211, val loss 0.1315, val f1 0.6391, lr 0.000300\n",
      "Epoch 126: train loss 0.0184, val loss 0.1718, val f1 0.6417, lr 0.000300\n",
      "Epoch 127: train loss 0.0186, val loss 0.1757, val f1 0.6391, lr 0.000300\n",
      "Epoch 128: train loss 0.0203, val loss 0.1468, val f1 0.6990, lr 0.000300\n",
      "Epoch 129: train loss 0.0263, val loss 0.1194, val f1 0.6594, lr 0.000300\n",
      "Epoch 130: train loss 0.0220, val loss 0.1505, val f1 0.6661, lr 0.000300\n",
      "Epoch 131: train loss 0.0219, val loss 0.1578, val f1 0.6477, lr 0.000300\n",
      "Epoch 132: train loss 0.0214, val loss 0.2000, val f1 0.6396, lr 0.000300\n",
      "Epoch 133: train loss 0.0257, val loss 0.1408, val f1 0.6627, lr 0.000300\n",
      "Epoch 134: train loss 0.0196, val loss 0.1481, val f1 0.6834, lr 0.000300\n",
      "Epoch 135: train loss 0.0174, val loss 0.1797, val f1 0.6290, lr 0.000300\n",
      "Epoch 136: train loss 0.0135, val loss 0.1847, val f1 0.6450, lr 0.000300\n",
      "Epoch 137: train loss 0.0162, val loss 0.1946, val f1 0.6404, lr 0.000300\n",
      "Epoch 138: train loss 0.0176, val loss 0.1828, val f1 0.6368, lr 0.000300\n",
      "Epoch 139: train loss 0.0160, val loss 0.1676, val f1 0.6589, lr 0.000300\n",
      "Epoch 140: train loss 0.0273, val loss 0.1508, val f1 0.6522, lr 0.000300\n",
      "Epoch 141: train loss 0.0142, val loss 0.1872, val f1 0.6691, lr 0.000300\n",
      "Epoch 142: train loss 0.0266, val loss 0.1532, val f1 0.6717, lr 0.000300\n",
      "Epoch 143: train loss 0.0144, val loss 0.1560, val f1 0.6576, lr 0.000300\n",
      "Epoch 144: train loss 0.0144, val loss 0.1653, val f1 0.6429, lr 0.000300\n",
      "Epoch 145: train loss 0.0173, val loss 0.1902, val f1 0.6506, lr 0.000300\n",
      "Epoch 146: train loss 0.0130, val loss 0.1700, val f1 0.6635, lr 0.000300\n",
      "Epoch 147: train loss 0.0205, val loss 0.1778, val f1 0.6404, lr 0.000300\n",
      "Epoch 148: train loss 0.0195, val loss 0.1675, val f1 0.6327, lr 0.000300\n",
      "Epoch 149: train loss 0.0168, val loss 0.1991, val f1 0.6550, lr 0.000300\n",
      "Epoch 150: train loss 0.0187, val loss 0.1601, val f1 0.6467, lr 0.000300\n",
      "Epoch 151: train loss 0.0158, val loss 0.1833, val f1 0.6489, lr 0.000300\n",
      "Epoch 152: train loss 0.0196, val loss 0.1546, val f1 0.6678, lr 0.000300\n",
      "Epoch 153: train loss 0.0105, val loss 0.1837, val f1 0.6578, lr 0.000300\n",
      "Epoch 154: train loss 0.0162, val loss 0.1673, val f1 0.6572, lr 0.000300\n",
      "Epoch 155: train loss 0.0121, val loss 0.1834, val f1 0.6450, lr 0.000300\n",
      "Epoch 156: train loss 0.0140, val loss 0.2194, val f1 0.5905, lr 0.000300\n",
      "Epoch 157: train loss 0.0199, val loss 0.1700, val f1 0.6670, lr 0.000300\n",
      "Epoch 158: train loss 0.0119, val loss 0.2267, val f1 0.6554, lr 0.000300\n",
      "Epoch 159: train loss 0.0135, val loss 0.1805, val f1 0.6678, lr 0.000300\n",
      "Epoch 160: train loss 0.0282, val loss 0.1914, val f1 0.6342, lr 0.000300\n",
      "Epoch 161: train loss 0.0205, val loss 0.1909, val f1 0.6249, lr 0.000300\n",
      "Epoch 162: train loss 0.0173, val loss 0.2026, val f1 0.6550, lr 0.000300\n",
      "Epoch 163: train loss 0.0121, val loss 0.1755, val f1 0.6579, lr 0.000300\n",
      "Epoch 164: train loss 0.0076, val loss 0.1999, val f1 0.6614, lr 0.000300\n",
      "Epoch 165: train loss 0.0239, val loss 0.1940, val f1 0.6760, lr 0.000300\n",
      "Epoch 166: train loss 0.0254, val loss 0.1876, val f1 0.6636, lr 0.000300\n",
      "Epoch 167: train loss 0.0140, val loss 0.1757, val f1 0.6735, lr 0.000300\n",
      "Epoch 168: train loss 0.0098, val loss 0.2162, val f1 0.6368, lr 0.000300\n",
      "Epoch 169: train loss 0.0090, val loss 0.1931, val f1 0.6740, lr 0.000300\n",
      "Epoch 170: train loss 0.0069, val loss 0.2235, val f1 0.6592, lr 0.000300\n",
      "Epoch 171: train loss 0.0092, val loss 0.2241, val f1 0.6462, lr 0.000300\n",
      "Epoch 172: train loss 0.0278, val loss 0.1631, val f1 0.6467, lr 0.000300\n",
      "Epoch 173: train loss 0.0134, val loss 0.1657, val f1 0.6848, lr 0.000300\n",
      "Epoch 174: train loss 0.0181, val loss 0.1995, val f1 0.6579, lr 0.000300\n",
      "Epoch 175: train loss 0.0095, val loss 0.1999, val f1 0.6508, lr 0.000300\n",
      "Epoch 176: train loss 0.0220, val loss 0.1437, val f1 0.6714, lr 0.000300\n",
      "Epoch 177: train loss 0.0124, val loss 0.1936, val f1 0.6564, lr 0.000300\n",
      "Epoch 178: train loss 0.0067, val loss 0.2038, val f1 0.6699, lr 0.000300\n",
      "Epoch 179: train loss 0.0068, val loss 0.2084, val f1 0.6974, lr 0.000300\n",
      "Epoch 180: train loss 0.0125, val loss 0.2165, val f1 0.6584, lr 0.000300\n",
      "Epoch 181: train loss 0.0131, val loss 0.2090, val f1 0.6821, lr 0.000300\n",
      "Epoch 182: train loss 0.0070, val loss 0.2364, val f1 0.6817, lr 0.000300\n",
      "Epoch 183: train loss 0.0301, val loss 0.1731, val f1 0.6365, lr 0.000300\n",
      "Epoch 184: train loss 0.0132, val loss 0.1868, val f1 0.6784, lr 0.000300\n",
      "Epoch 185: train loss 0.0079, val loss 0.1977, val f1 0.6562, lr 0.000300\n",
      "Epoch 186: train loss 0.0143, val loss 0.2036, val f1 0.6745, lr 0.000300\n",
      "Epoch 187: train loss 0.0166, val loss 0.1855, val f1 0.6589, lr 0.000300\n",
      "Epoch 188: train loss 0.0213, val loss 0.1842, val f1 0.6438, lr 0.000300\n",
      "Epoch 189: train loss 0.0126, val loss 0.1957, val f1 0.6511, lr 0.000300\n",
      "Epoch 190: train loss 0.0107, val loss 0.2016, val f1 0.6691, lr 0.000300\n",
      "Epoch 191: train loss 0.0114, val loss 0.1972, val f1 0.6561, lr 0.000300\n",
      "Epoch 192: train loss 0.0067, val loss 0.2101, val f1 0.6458, lr 0.000300\n",
      "Epoch 193: train loss 0.0059, val loss 0.2412, val f1 0.6699, lr 0.000300\n",
      "Epoch 194: train loss 0.0064, val loss 0.2266, val f1 0.6562, lr 0.000300\n",
      "Epoch 195: train loss 0.0188, val loss 0.2080, val f1 0.6787, lr 0.000300\n",
      "Epoch 196: train loss 0.0140, val loss 0.2193, val f1 0.6508, lr 0.000300\n",
      "Epoch 197: train loss 0.0206, val loss 0.1862, val f1 0.6454, lr 0.000300\n",
      "Epoch 198: train loss 0.0160, val loss 0.1997, val f1 0.6074, lr 0.000300\n",
      "Epoch 199: train loss 0.0118, val loss 0.1983, val f1 0.6603, lr 0.000300\n",
      "Epoch 200: train loss 0.0080, val loss 0.2227, val f1 0.6508, lr 0.000300\n",
      "Epoch 201: train loss 0.0118, val loss 0.1940, val f1 0.6511, lr 0.000300\n",
      "Epoch 202: train loss 0.0065, val loss 0.2293, val f1 0.6909, lr 0.000300\n",
      "Epoch 203: train loss 0.0149, val loss 0.2104, val f1 0.6803, lr 0.000300\n",
      "Epoch 204: train loss 0.0080, val loss 0.2284, val f1 0.6632, lr 0.000300\n",
      "Epoch 205: train loss 0.0057, val loss 0.2354, val f1 0.6735, lr 0.000300\n",
      "Epoch 206: train loss 0.0050, val loss 0.2329, val f1 0.6684, lr 0.000300\n",
      "Epoch 207: train loss 0.0052, val loss 0.2473, val f1 0.6847, lr 0.000300\n",
      "Epoch 208: train loss 0.0094, val loss 0.2573, val f1 0.6676, lr 0.000300\n",
      "Epoch 209: train loss 0.0308, val loss 0.1820, val f1 0.6489, lr 0.000300\n",
      "Epoch 210: train loss 0.0174, val loss 0.1854, val f1 0.6481, lr 0.000300\n",
      "Epoch 211: train loss 0.0096, val loss 0.1936, val f1 0.6893, lr 0.000300\n",
      "Epoch 212: train loss 0.0068, val loss 0.2109, val f1 0.6417, lr 0.000300\n",
      "Epoch 213: train loss 0.0048, val loss 0.2180, val f1 0.6664, lr 0.000300\n",
      "Epoch 214: train loss 0.0054, val loss 0.2468, val f1 0.6621, lr 0.000300\n",
      "Epoch 215: train loss 0.0159, val loss 0.2047, val f1 0.6744, lr 0.000300\n",
      "Epoch 216: train loss 0.0224, val loss 0.2044, val f1 0.6458, lr 0.000300\n",
      "Epoch 217: train loss 0.0207, val loss 0.2144, val f1 0.6352, lr 0.000300\n",
      "Epoch 218: train loss 0.0079, val loss 0.2304, val f1 0.6578, lr 0.000300\n",
      "Epoch 219: train loss 0.0075, val loss 0.2358, val f1 0.6684, lr 0.000300\n",
      "Epoch 220: train loss 0.0056, val loss 0.2555, val f1 0.6720, lr 0.000300\n",
      "Epoch 221: train loss 0.0155, val loss 0.1912, val f1 0.6828, lr 0.000300\n",
      "Epoch 222: train loss 0.0060, val loss 0.2042, val f1 0.6837, lr 0.000300\n",
      "Epoch 223: train loss 0.0166, val loss 0.2087, val f1 0.6665, lr 0.000300\n",
      "Epoch 224: train loss 0.0104, val loss 0.2078, val f1 0.6692, lr 0.000300\n",
      "Epoch 225: train loss 0.0141, val loss 0.2014, val f1 0.6627, lr 0.000300\n",
      "Epoch 226: train loss 0.0071, val loss 0.2150, val f1 0.6799, lr 0.000300\n",
      "Epoch 227: train loss 0.0055, val loss 0.2271, val f1 0.6760, lr 0.000300\n",
      "Epoch 228: train loss 0.0060, val loss 0.2403, val f1 0.6846, lr 0.000300\n",
      "Epoch 229: train loss 0.0049, val loss 0.2552, val f1 0.6776, lr 0.000300\n",
      "Epoch 230: train loss 0.0045, val loss 0.2630, val f1 0.6621, lr 0.000300\n",
      "Epoch 231: train loss 0.0051, val loss 0.2635, val f1 0.6828, lr 0.000300\n",
      "Epoch 232: train loss 0.0045, val loss 0.2768, val f1 0.6536, lr 0.000300\n",
      "Epoch 233: train loss 0.0105, val loss 0.2340, val f1 0.6437, lr 0.000300\n",
      "Epoch 234: train loss 0.0264, val loss 0.2280, val f1 0.6193, lr 0.000300\n",
      "Epoch 235: train loss 0.0117, val loss 0.2405, val f1 0.6437, lr 0.000300\n",
      "Epoch 236: train loss 0.0075, val loss 0.2257, val f1 0.6483, lr 0.000300\n",
      "Epoch 237: train loss 0.0056, val loss 0.2587, val f1 0.6676, lr 0.000300\n",
      "Epoch 238: train loss 0.0039, val loss 0.2560, val f1 0.6755, lr 0.000300\n",
      "Epoch 239: train loss 0.0077, val loss 0.2568, val f1 0.6685, lr 0.000300\n",
      "Epoch 240: train loss 0.0057, val loss 0.3256, val f1 0.6503, lr 0.000300\n",
      "Epoch 241: train loss 0.0166, val loss 0.2458, val f1 0.6664, lr 0.000300\n",
      "Epoch 242: train loss 0.0059, val loss 0.2644, val f1 0.6664, lr 0.000300\n",
      "Epoch 243: train loss 0.0044, val loss 0.2760, val f1 0.6720, lr 0.000300\n",
      "Epoch 244: train loss 0.0050, val loss 0.2808, val f1 0.6878, lr 0.000300\n",
      "Epoch 245: train loss 0.0191, val loss 0.2129, val f1 0.6623, lr 0.000300\n",
      "Epoch 246: train loss 0.0097, val loss 0.2147, val f1 0.6684, lr 0.000300\n",
      "Epoch 247: train loss 0.0193, val loss 0.2005, val f1 0.6454, lr 0.000300\n",
      "Epoch 248: train loss 0.0061, val loss 0.2294, val f1 0.6550, lr 0.000300\n",
      "Epoch 249: train loss 0.0039, val loss 0.2390, val f1 0.7003, lr 0.000300\n",
      "Epoch 250: train loss 0.0044, val loss 0.2437, val f1 0.6750, lr 0.000300\n",
      "Epoch 251: train loss 0.0037, val loss 0.2549, val f1 0.6780, lr 0.000300\n",
      "Epoch 252: train loss 0.0043, val loss 0.2735, val f1 0.6819, lr 0.000300\n",
      "Epoch 253: train loss 0.0271, val loss 0.2451, val f1 0.6661, lr 0.000300\n",
      "Epoch 254: train loss 0.0104, val loss 0.2108, val f1 0.6740, lr 0.000300\n",
      "Epoch 255: train loss 0.0067, val loss 0.2327, val f1 0.6755, lr 0.000300\n",
      "Epoch 256: train loss 0.0038, val loss 0.2563, val f1 0.6576, lr 0.000300\n",
      "Epoch 257: train loss 0.0032, val loss 0.2649, val f1 0.6698, lr 0.000300\n",
      "Epoch 258: train loss 0.0032, val loss 0.2857, val f1 0.6684, lr 0.000300\n",
      "Epoch 259: train loss 0.0043, val loss 0.2917, val f1 0.6755, lr 0.000300\n",
      "Epoch 260: train loss 0.0079, val loss 0.2930, val f1 0.6726, lr 0.000300\n",
      "Epoch 261: train loss 0.0042, val loss 0.2906, val f1 0.6740, lr 0.000300\n",
      "Epoch 262: train loss 0.0038, val loss 0.3076, val f1 0.6685, lr 0.000300\n",
      "Epoch 263: train loss 0.0068, val loss 0.3176, val f1 0.6422, lr 0.000300\n",
      "Epoch 264: train loss 0.0050, val loss 0.3292, val f1 0.6496, lr 0.000300\n",
      "Epoch 265: train loss 0.0041, val loss 0.3140, val f1 0.6603, lr 0.000300\n",
      "Epoch 266: train loss 0.0033, val loss 0.3326, val f1 0.6627, lr 0.000300\n",
      "Epoch 267: train loss 0.0490, val loss 0.1992, val f1 0.6658, lr 0.000300\n",
      "Epoch 268: train loss 0.0102, val loss 0.2066, val f1 0.6766, lr 0.000300\n",
      "Epoch 269: train loss 0.0076, val loss 0.2444, val f1 0.6646, lr 0.000300\n",
      "Epoch 270: train loss 0.0056, val loss 0.2571, val f1 0.6815, lr 0.000300\n",
      "Epoch 271: train loss 0.0078, val loss 0.2537, val f1 0.6765, lr 0.000300\n",
      "Epoch 272: train loss 0.0043, val loss 0.2637, val f1 0.6706, lr 0.000300\n",
      "Epoch 273: train loss 0.0239, val loss 0.2053, val f1 0.6523, lr 0.000300\n",
      "Epoch 274: train loss 0.0189, val loss 0.1856, val f1 0.6548, lr 0.000300\n",
      "Epoch 275: train loss 0.0057, val loss 0.2128, val f1 0.6637, lr 0.000300\n",
      "Epoch 276: train loss 0.0040, val loss 0.2335, val f1 0.6522, lr 0.000300\n",
      "Epoch 277: train loss 0.0056, val loss 0.2566, val f1 0.6661, lr 0.000300\n",
      "Epoch 278: train loss 0.0116, val loss 0.2413, val f1 0.6692, lr 0.000300\n",
      "Epoch 279: train loss 0.0047, val loss 0.2678, val f1 0.6562, lr 0.000300\n",
      "Epoch 280: train loss 0.0085, val loss 0.2743, val f1 0.6603, lr 0.000300\n",
      "Epoch 281: train loss 0.0049, val loss 0.3132, val f1 0.6550, lr 0.000300\n",
      "Epoch 282: train loss 0.0080, val loss 0.2715, val f1 0.6654, lr 0.000300\n",
      "Epoch 283: train loss 0.0088, val loss 0.2258, val f1 0.6665, lr 0.000300\n",
      "Epoch 284: train loss 0.0117, val loss 0.2589, val f1 0.6536, lr 0.000300\n",
      "Epoch 285: train loss 0.0062, val loss 0.2570, val f1 0.6627, lr 0.000300\n",
      "Epoch 286: train loss 0.0032, val loss 0.2566, val f1 0.6805, lr 0.000300\n",
      "Epoch 287: train loss 0.0036, val loss 0.2648, val f1 0.6863, lr 0.000300\n",
      "Epoch 288: train loss 0.0036, val loss 0.2821, val f1 0.6894, lr 0.000300\n",
      "Epoch 289: train loss 0.0037, val loss 0.2956, val f1 0.6684, lr 0.000300\n",
      "Epoch 290: train loss 0.0036, val loss 0.3059, val f1 0.6536, lr 0.000300\n",
      "Epoch 291: train loss 0.0268, val loss 0.2804, val f1 0.6365, lr 0.000300\n",
      "Epoch 292: train loss 0.0409, val loss 0.1848, val f1 0.7128, lr 0.000300\n",
      "Epoch 293: train loss 0.0062, val loss 0.2001, val f1 0.6614, lr 0.000300\n",
      "Epoch 294: train loss 0.0086, val loss 0.2263, val f1 0.6563, lr 0.000300\n",
      "Epoch 295: train loss 0.0044, val loss 0.2380, val f1 0.6632, lr 0.000300\n",
      "Epoch 296: train loss 0.0038, val loss 0.2425, val f1 0.6834, lr 0.000300\n",
      "Epoch 297: train loss 0.0055, val loss 0.2824, val f1 0.6642, lr 0.000300\n",
      "Epoch 298: train loss 0.0047, val loss 0.2586, val f1 0.6799, lr 0.000300\n",
      "Epoch 299: train loss 0.0034, val loss 0.2815, val f1 0.6776, lr 0.000300\n",
      "Epoch 300: train loss 0.0105, val loss 0.2533, val f1 0.6550, lr 0.000300\n",
      "Epoch 301: train loss 0.0145, val loss 0.2569, val f1 0.6343, lr 0.000300\n",
      "Epoch 302: train loss 0.0144, val loss 0.2376, val f1 0.6600, lr 0.000300\n",
      "Epoch 303: train loss 0.0066, val loss 0.2398, val f1 0.6676, lr 0.000300\n",
      "Epoch 304: train loss 0.0034, val loss 0.2560, val f1 0.6862, lr 0.000300\n",
      "Epoch 305: train loss 0.0029, val loss 0.2568, val f1 0.6830, lr 0.000300\n",
      "Epoch 306: train loss 0.0038, val loss 0.2589, val f1 0.6830, lr 0.000300\n",
      "Epoch 307: train loss 0.0099, val loss 0.2668, val f1 0.6766, lr 0.000300\n",
      "Epoch 308: train loss 0.0090, val loss 0.2726, val f1 0.7066, lr 0.000300\n",
      "Epoch 309: train loss 0.0080, val loss 0.2692, val f1 0.6589, lr 0.000300\n",
      "Epoch 310: train loss 0.0058, val loss 0.2639, val f1 0.6559, lr 0.000300\n",
      "Epoch 311: train loss 0.0052, val loss 0.2527, val f1 0.6603, lr 0.000300\n",
      "Epoch 312: train loss 0.0053, val loss 0.2995, val f1 0.6651, lr 0.000300\n",
      "Epoch 313: train loss 0.0205, val loss 0.2266, val f1 0.6837, lr 0.000300\n",
      "Epoch 314: train loss 0.0090, val loss 0.2600, val f1 0.6664, lr 0.000300\n",
      "Epoch 315: train loss 0.0035, val loss 0.2757, val f1 0.6670, lr 0.000300\n",
      "Epoch 316: train loss 0.0043, val loss 0.2885, val f1 0.6714, lr 0.000300\n",
      "Epoch 317: train loss 0.0023, val loss 0.2930, val f1 0.6578, lr 0.000300\n",
      "Epoch 318: train loss 0.0252, val loss 0.2545, val f1 0.6520, lr 0.000300\n",
      "Epoch 319: train loss 0.0059, val loss 0.2387, val f1 0.6766, lr 0.000300\n",
      "Epoch 320: train loss 0.0044, val loss 0.2722, val f1 0.6830, lr 0.000300\n",
      "Epoch 321: train loss 0.0063, val loss 0.2562, val f1 0.6661, lr 0.000300\n",
      "Epoch 322: train loss 0.0062, val loss 0.2500, val f1 0.6712, lr 0.000300\n",
      "Epoch 323: train loss 0.0041, val loss 0.2794, val f1 0.6765, lr 0.000300\n",
      "Epoch 324: train loss 0.0049, val loss 0.2774, val f1 0.6908, lr 0.000300\n",
      "Epoch 325: train loss 0.0026, val loss 0.2954, val f1 0.6617, lr 0.000300\n",
      "Epoch 326: train loss 0.0026, val loss 0.2847, val f1 0.6643, lr 0.000300\n",
      "Epoch 327: train loss 0.0021, val loss 0.2980, val f1 0.6729, lr 0.000300\n",
      "Epoch 328: train loss 0.0039, val loss 0.2693, val f1 0.6788, lr 0.000300\n",
      "Epoch 329: train loss 0.0157, val loss 0.2443, val f1 0.6603, lr 0.000300\n",
      "Epoch 330: train loss 0.0147, val loss 0.2245, val f1 0.6658, lr 0.000300\n",
      "Epoch 331: train loss 0.0039, val loss 0.2455, val f1 0.6720, lr 0.000300\n",
      "Epoch 332: train loss 0.0030, val loss 0.2763, val f1 0.6610, lr 0.000300\n",
      "Epoch 333: train loss 0.0161, val loss 0.2324, val f1 0.6212, lr 0.000300\n",
      "Epoch 334: train loss 0.0119, val loss 0.2392, val f1 0.6487, lr 0.000300\n",
      "Epoch 335: train loss 0.0050, val loss 0.2804, val f1 0.6563, lr 0.000300\n",
      "Epoch 336: train loss 0.0034, val loss 0.2790, val f1 0.6784, lr 0.000300\n",
      "Epoch 337: train loss 0.0047, val loss 0.2968, val f1 0.6646, lr 0.000300\n",
      "Epoch 338: train loss 0.0032, val loss 0.3113, val f1 0.6699, lr 0.000300\n",
      "Epoch 339: train loss 0.0032, val loss 0.3177, val f1 0.6684, lr 0.000300\n",
      "Epoch 340: train loss 0.0026, val loss 0.3007, val f1 0.6745, lr 0.000300\n",
      "Epoch 341: train loss 0.0048, val loss 0.4219, val f1 0.6344, lr 0.000300\n",
      "Epoch 342: train loss 0.0331, val loss 0.2156, val f1 0.6481, lr 0.000300\n",
      "Epoch 343: train loss 0.0113, val loss 0.2269, val f1 0.6458, lr 0.000300\n",
      "Epoch 344: train loss 0.0124, val loss 0.2188, val f1 0.6719, lr 0.000300\n",
      "Epoch 345: train loss 0.0040, val loss 0.2790, val f1 0.6550, lr 0.000300\n",
      "Epoch 346: train loss 0.0028, val loss 0.2657, val f1 0.6765, lr 0.000300\n",
      "Epoch 347: train loss 0.0025, val loss 0.2776, val f1 0.6780, lr 0.000300\n",
      "Epoch 348: train loss 0.0120, val loss 0.2953, val f1 0.6163, lr 0.000300\n",
      "Epoch 349: train loss 0.0108, val loss 0.2577, val f1 0.6339, lr 0.000300\n",
      "Epoch 350: train loss 0.0060, val loss 0.2529, val f1 0.6481, lr 0.000300\n",
      "Epoch 351: train loss 0.0040, val loss 0.2739, val f1 0.6437, lr 0.000300\n",
      "Epoch 352: train loss 0.0034, val loss 0.2847, val f1 0.6411, lr 0.000300\n",
      "Epoch 353: train loss 0.0054, val loss 0.2887, val f1 0.6523, lr 0.000300\n",
      "Epoch 354: train loss 0.0179, val loss 0.2658, val f1 0.6330, lr 0.000300\n",
      "Epoch 355: train loss 0.0091, val loss 0.2769, val f1 0.6506, lr 0.000300\n",
      "Epoch 356: train loss 0.0107, val loss 0.2409, val f1 0.6523, lr 0.000300\n",
      "Epoch 357: train loss 0.0044, val loss 0.2546, val f1 0.6483, lr 0.000300\n",
      "Epoch 358: train loss 0.0111, val loss 0.2377, val f1 0.6658, lr 0.000300\n",
      "Epoch 359: train loss 0.0052, val loss 0.2592, val f1 0.6496, lr 0.000300\n",
      "Epoch 360: train loss 0.0036, val loss 0.2796, val f1 0.6522, lr 0.000300\n",
      "Epoch 361: train loss 0.0033, val loss 0.2997, val f1 0.6450, lr 0.000300\n",
      "Epoch 362: train loss 0.0029, val loss 0.3041, val f1 0.6578, lr 0.000300\n",
      "Epoch 363: train loss 0.0025, val loss 0.3021, val f1 0.6509, lr 0.000300\n",
      "Epoch 364: train loss 0.0022, val loss 0.3122, val f1 0.6481, lr 0.000300\n",
      "Epoch 365: train loss 0.0022, val loss 0.3354, val f1 0.6477, lr 0.000300\n",
      "Epoch 366: train loss 0.0024, val loss 0.3235, val f1 0.6576, lr 0.000300\n",
      "Epoch 367: train loss 0.0191, val loss 0.2398, val f1 0.6548, lr 0.000300\n",
      "Epoch 368: train loss 0.0138, val loss 0.2559, val f1 0.6342, lr 0.000300\n",
      "Epoch 369: train loss 0.0053, val loss 0.2684, val f1 0.6352, lr 0.000300\n",
      "Epoch 370: train loss 0.0048, val loss 0.2917, val f1 0.6576, lr 0.000300\n",
      "Epoch 371: train loss 0.0037, val loss 0.3027, val f1 0.6385, lr 0.000300\n",
      "Epoch 372: train loss 0.0037, val loss 0.3220, val f1 0.6765, lr 0.000300\n",
      "Epoch 373: train loss 0.0037, val loss 0.3345, val f1 0.6377, lr 0.000300\n",
      "Epoch 374: train loss 0.0181, val loss 0.2331, val f1 0.6508, lr 0.000300\n",
      "Epoch 375: train loss 0.0226, val loss 0.2312, val f1 0.6587, lr 0.000300\n",
      "Epoch 376: train loss 0.0084, val loss 0.2364, val f1 0.6477, lr 0.000300\n",
      "Epoch 377: train loss 0.0041, val loss 0.2622, val f1 0.6404, lr 0.000300\n",
      "Epoch 378: train loss 0.0038, val loss 0.2677, val f1 0.6483, lr 0.000300\n",
      "Epoch 379: train loss 0.0032, val loss 0.2792, val f1 0.6442, lr 0.000300\n",
      "Epoch 380: train loss 0.0032, val loss 0.2887, val f1 0.6467, lr 0.000300\n",
      "Epoch 381: train loss 0.0028, val loss 0.2890, val f1 0.6712, lr 0.000300\n",
      "Epoch 382: train loss 0.0057, val loss 0.2967, val f1 0.6572, lr 0.000300\n",
      "Epoch 383: train loss 0.0050, val loss 0.2938, val f1 0.6615, lr 0.000300\n",
      "Epoch 384: train loss 0.0079, val loss 0.3072, val f1 0.6424, lr 0.000300\n",
      "Epoch 385: train loss 0.0051, val loss 0.3055, val f1 0.6471, lr 0.000300\n",
      "Epoch 386: train loss 0.0034, val loss 0.3365, val f1 0.6564, lr 0.000300\n",
      "Epoch 387: train loss 0.0028, val loss 0.3505, val f1 0.6592, lr 0.000300\n",
      "Epoch 388: train loss 0.0031, val loss 0.3704, val f1 0.6745, lr 0.000300\n",
      "Epoch 389: train loss 0.0026, val loss 0.3950, val f1 0.6522, lr 0.000300\n",
      "Epoch 390: train loss 0.0390, val loss 0.2269, val f1 0.6059, lr 0.000300\n",
      "Epoch 391: train loss 0.0176, val loss 0.2278, val f1 0.6361, lr 0.000300\n",
      "Epoch 392: train loss 0.0122, val loss 0.2725, val f1 0.6454, lr 0.000300\n",
      "Epoch 393: train loss 0.0047, val loss 0.2643, val f1 0.6449, lr 0.000300\n",
      "Epoch 394: train loss 0.0036, val loss 0.2848, val f1 0.6664, lr 0.000300\n",
      "Epoch 395: train loss 0.0049, val loss 0.2788, val f1 0.6784, lr 0.000300\n",
      "Epoch 396: train loss 0.0030, val loss 0.2813, val f1 0.6740, lr 0.000300\n",
      "Epoch 397: train loss 0.0047, val loss 0.3189, val f1 0.6417, lr 0.000300\n",
      "Epoch 398: train loss 0.0028, val loss 0.3104, val f1 0.6646, lr 0.000300\n",
      "Epoch 399: train loss 0.0045, val loss 0.3119, val f1 0.6606, lr 0.000300\n",
      "Epoch 400: train loss 0.0030, val loss 0.3092, val f1 0.6589, lr 0.000300\n",
      "Epoch 401: train loss 0.0017, val loss 0.3432, val f1 0.6522, lr 0.000300\n",
      "Epoch 402: train loss 0.0027, val loss 0.3270, val f1 0.6606, lr 0.000300\n",
      "Epoch 403: train loss 0.0022, val loss 0.3367, val f1 0.6655, lr 0.000300\n",
      "Epoch 404: train loss 0.0022, val loss 0.3389, val f1 0.6698, lr 0.000300\n",
      "Epoch 405: train loss 0.0024, val loss 0.3527, val f1 0.6848, lr 0.000300\n",
      "Epoch 406: train loss 0.0018, val loss 0.3696, val f1 0.6714, lr 0.000300\n",
      "Epoch 407: train loss 0.0025, val loss 0.3550, val f1 0.6759, lr 0.000300\n",
      "Epoch 408: train loss 0.0049, val loss 0.3885, val f1 0.6925, lr 0.000300\n",
      "Epoch 409: train loss 0.0400, val loss 0.2841, val f1 0.5996, lr 0.000300\n",
      "Epoch 410: train loss 0.0180, val loss 0.2538, val f1 0.6755, lr 0.000300\n",
      "Epoch 411: train loss 0.0051, val loss 0.2765, val f1 0.6720, lr 0.000300\n",
      "Epoch 412: train loss 0.0032, val loss 0.3073, val f1 0.6745, lr 0.000300\n",
      "Epoch 413: train loss 0.0042, val loss 0.2766, val f1 0.6684, lr 0.000300\n",
      "Epoch 414: train loss 0.0067, val loss 0.3039, val f1 0.6650, lr 0.000300\n",
      "Epoch 415: train loss 0.0070, val loss 0.3156, val f1 0.6606, lr 0.000300\n",
      "Epoch 416: train loss 0.0036, val loss 0.3129, val f1 0.6812, lr 0.000300\n",
      "Epoch 417: train loss 0.0031, val loss 0.3166, val f1 0.6706, lr 0.000300\n",
      "Epoch 418: train loss 0.0026, val loss 0.3384, val f1 0.6765, lr 0.000300\n",
      "Epoch 419: train loss 0.0040, val loss 0.3520, val f1 0.6606, lr 0.000300\n",
      "Epoch 420: train loss 0.0031, val loss 0.3430, val f1 0.6564, lr 0.000300\n",
      "Epoch 421: train loss 0.0024, val loss 0.3526, val f1 0.6606, lr 0.000300\n",
      "Epoch 422: train loss 0.0043, val loss 0.3503, val f1 0.6592, lr 0.000300\n",
      "Epoch 423: train loss 0.0025, val loss 0.3753, val f1 0.6676, lr 0.000300\n",
      "Epoch 424: train loss 0.0016, val loss 0.3571, val f1 0.6780, lr 0.000300\n",
      "Epoch 425: train loss 0.0022, val loss 0.3844, val f1 0.6467, lr 0.000300\n",
      "Epoch 426: train loss 0.0070, val loss 0.3822, val f1 0.6446, lr 0.000300\n",
      "Epoch 427: train loss 0.0187, val loss 0.3247, val f1 0.6615, lr 0.000300\n",
      "Epoch 428: train loss 0.0097, val loss 0.2895, val f1 0.6678, lr 0.000300\n",
      "Epoch 429: train loss 0.0035, val loss 0.3328, val f1 0.6828, lr 0.000300\n",
      "Epoch 430: train loss 0.0026, val loss 0.3346, val f1 0.6828, lr 0.000300\n",
      "Epoch 431: train loss 0.0026, val loss 0.3084, val f1 0.6637, lr 0.000300\n",
      "Epoch 432: train loss 0.0130, val loss 0.3785, val f1 0.6536, lr 0.000300\n",
      "Epoch 433: train loss 0.0042, val loss 0.3382, val f1 0.6587, lr 0.000300\n",
      "Epoch 434: train loss 0.0031, val loss 0.3416, val f1 0.6710, lr 0.000300\n",
      "Epoch 435: train loss 0.0070, val loss 0.3152, val f1 0.6875, lr 0.000300\n",
      "Epoch 436: train loss 0.0059, val loss 0.2760, val f1 0.6740, lr 0.000300\n",
      "Epoch 437: train loss 0.0117, val loss 0.2876, val f1 0.6669, lr 0.000300\n",
      "Epoch 438: train loss 0.0140, val loss 0.3058, val f1 0.6352, lr 0.000300\n",
      "Epoch 439: train loss 0.0129, val loss 0.2489, val f1 0.6810, lr 0.000300\n",
      "Epoch 440: train loss 0.0047, val loss 0.2785, val f1 0.6562, lr 0.000300\n",
      "Epoch 441: train loss 0.0044, val loss 0.3088, val f1 0.6769, lr 0.000300\n",
      "Epoch 442: train loss 0.0026, val loss 0.2929, val f1 0.6712, lr 0.000300\n",
      "Epoch 443: train loss 0.0032, val loss 0.3122, val f1 0.6772, lr 0.000300\n",
      "Epoch 444: train loss 0.0085, val loss 0.2938, val f1 0.6863, lr 0.000300\n",
      "Epoch 445: train loss 0.0070, val loss 0.3480, val f1 0.6729, lr 0.000300\n",
      "Epoch 446: train loss 0.0032, val loss 0.3288, val f1 0.6750, lr 0.000300\n",
      "Epoch 447: train loss 0.0041, val loss 0.3400, val f1 0.6641, lr 0.000300\n",
      "Epoch 448: train loss 0.0026, val loss 0.3447, val f1 0.6684, lr 0.000300\n",
      "Epoch 449: train loss 0.0055, val loss 0.3320, val f1 0.6740, lr 0.000300\n",
      "Epoch 450: train loss 0.0028, val loss 0.3433, val f1 0.6655, lr 0.000300\n",
      "Epoch 451: train loss 0.0025, val loss 0.3519, val f1 0.6755, lr 0.000300\n",
      "Epoch 452: train loss 0.0056, val loss 0.3192, val f1 0.6670, lr 0.000300\n",
      "Epoch 453: train loss 0.0032, val loss 0.2993, val f1 0.6712, lr 0.000300\n",
      "Epoch 454: train loss 0.0205, val loss 0.2561, val f1 0.6815, lr 0.000300\n",
      "Epoch 455: train loss 0.0127, val loss 0.2582, val f1 0.6641, lr 0.000300\n",
      "Epoch 456: train loss 0.0031, val loss 0.2794, val f1 0.6536, lr 0.000300\n",
      "Epoch 457: train loss 0.0042, val loss 0.2846, val f1 0.6735, lr 0.000300\n",
      "Epoch 458: train loss 0.0026, val loss 0.2802, val f1 0.6720, lr 0.000300\n",
      "Epoch 459: train loss 0.0023, val loss 0.3033, val f1 0.6676, lr 0.000300\n",
      "Epoch 460: train loss 0.0018, val loss 0.2828, val f1 0.6994, lr 0.000300\n",
      "Epoch 461: train loss 0.0019, val loss 0.2974, val f1 0.6769, lr 0.000300\n",
      "Epoch 462: train loss 0.0016, val loss 0.3134, val f1 0.6589, lr 0.000300\n",
      "Epoch 463: train loss 0.0013, val loss 0.3219, val f1 0.6830, lr 0.000300\n",
      "Epoch 464: train loss 0.0014, val loss 0.3457, val f1 0.6578, lr 0.000300\n",
      "Epoch 465: train loss 0.0065, val loss 0.3198, val f1 0.6691, lr 0.000300\n",
      "Epoch 466: train loss 0.0027, val loss 0.3459, val f1 0.6671, lr 0.000300\n",
      "Epoch 467: train loss 0.0073, val loss 0.4555, val f1 0.6166, lr 0.000300\n",
      "Epoch 468: train loss 0.0238, val loss 0.2978, val f1 0.6679, lr 0.000300\n",
      "Epoch 469: train loss 0.0047, val loss 0.3281, val f1 0.6707, lr 0.000300\n",
      "Epoch 470: train loss 0.0160, val loss 0.3193, val f1 0.6471, lr 0.000300\n",
      "Epoch 471: train loss 0.0036, val loss 0.3006, val f1 0.6835, lr 0.000300\n",
      "Epoch 472: train loss 0.0023, val loss 0.3264, val f1 0.6799, lr 0.000300\n",
      "Epoch 473: train loss 0.0021, val loss 0.3449, val f1 0.6699, lr 0.000300\n",
      "Epoch 474: train loss 0.0021, val loss 0.3461, val f1 0.6750, lr 0.000300\n",
      "Epoch 475: train loss 0.0061, val loss 0.3394, val f1 0.6603, lr 0.000300\n",
      "Epoch 476: train loss 0.0026, val loss 0.3644, val f1 0.6670, lr 0.000300\n",
      "Epoch 477: train loss 0.0021, val loss 0.3460, val f1 0.6710, lr 0.000300\n",
      "Epoch 478: train loss 0.0020, val loss 0.4313, val f1 0.6536, lr 0.000300\n",
      "Epoch 479: train loss 0.0042, val loss 0.3991, val f1 0.6578, lr 0.000300\n",
      "Epoch 480: train loss 0.0188, val loss 0.2850, val f1 0.6392, lr 0.000300\n",
      "Epoch 481: train loss 0.0093, val loss 0.3292, val f1 0.6655, lr 0.000300\n",
      "Epoch 482: train loss 0.0035, val loss 0.2909, val f1 0.6807, lr 0.000300\n",
      "Epoch 483: train loss 0.0074, val loss 0.3151, val f1 0.6832, lr 0.000300\n",
      "Epoch 484: train loss 0.0016, val loss 0.3310, val f1 0.6720, lr 0.000300\n",
      "Epoch 485: train loss 0.0015, val loss 0.3431, val f1 0.6632, lr 0.000300\n",
      "Epoch 486: train loss 0.0013, val loss 0.3395, val f1 0.6847, lr 0.000300\n",
      "Epoch 487: train loss 0.0131, val loss 0.3325, val f1 0.6450, lr 0.000300\n",
      "Epoch 488: train loss 0.0054, val loss 0.3065, val f1 0.6908, lr 0.000300\n",
      "Epoch 489: train loss 0.0021, val loss 0.3133, val f1 0.6706, lr 0.000300\n",
      "Epoch 490: train loss 0.0019, val loss 0.3440, val f1 0.6684, lr 0.000300\n",
      "Epoch 491: train loss 0.0033, val loss 0.3771, val f1 0.6431, lr 0.000300\n",
      "Epoch 492: train loss 0.0020, val loss 0.3373, val f1 0.6894, lr 0.000300\n",
      "Epoch 493: train loss 0.0109, val loss 0.3009, val f1 0.6817, lr 0.000300\n",
      "Epoch 494: train loss 0.0122, val loss 0.2689, val f1 0.6817, lr 0.000300\n",
      "Epoch 495: train loss 0.0031, val loss 0.3284, val f1 0.6878, lr 0.000300\n",
      "Epoch 496: train loss 0.0071, val loss 0.3338, val f1 0.6564, lr 0.000300\n",
      "Epoch 497: train loss 0.0052, val loss 0.3408, val f1 0.6878, lr 0.000300\n",
      "Epoch 498: train loss 0.0022, val loss 0.3394, val f1 0.6799, lr 0.000300\n",
      "Epoch 499: train loss 0.0020, val loss 0.3548, val f1 0.6846, lr 0.000300\n"
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "val_lossi = []\n",
    "val_f1i = []\n",
    "val_i = []\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "model.train()\n",
    "\n",
    "model.to('cuda')\n",
    "criterion = criterion.to('cuda')\n",
    "\n",
    "for _ in range(500):\n",
    "    loss_epoch = 0\n",
    "    for Xi,yi in trainloader:\n",
    "        Xi = Xi.to('cuda')\n",
    "        yi = yi.to('cuda')\n",
    "        logits = model(Xi)\n",
    "        loss = criterion(logits, yi)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "    loss_epoch /= len(trainloader)\n",
    "    lossi.append(loss_epoch)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for Xi, yi in valloader:\n",
    "            Xi = Xi.to('cuda')\n",
    "            yi = yi.to('cuda')\n",
    "            logits = model(Xi)\n",
    "            loss = criterion(logits, yi)\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            all_preds.append(torch.sigmoid(logits).round().cpu())\n",
    "            all_labels.append(yi.cpu())\n",
    "\n",
    "    val_loss = val_loss_total / len(valloader)\n",
    "    val_f1 = f1_score(\n",
    "    torch.cat(all_labels).numpy(),\n",
    "    torch.cat(all_preds).numpy(),\n",
    "    average='macro'\n",
    "    )\n",
    "\n",
    "    val_lossi.append(val_loss)\n",
    "    val_i.append(epoch)\n",
    "    val_f1i.append(val_f1)\n",
    "\n",
    "    fig,ax= plt.subplots(nrows=2,ncols=1,figsize=(7.2,10))\n",
    "    ax[0].plot(lossi)\n",
    "    ax[0].plot(val_i, val_lossi, color='red')\n",
    "    ax[0].set_yscale('log')\n",
    "\n",
    "    ax[1].plot(val_i, val_f1i, color='green')\n",
    "\n",
    "    plt.savefig(f'loss.png')\n",
    "    plt.close()\n",
    "        \n",
    "    print(f'Epoch {epoch}: train loss {loss_epoch:.4f}, val loss {val_loss:.4f}, val f1 {val_f1:.4f}, lr {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    model.train()\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90146c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling = f'andrew smoking labels'\n",
    "window_stride_eval = 3000\n",
    "sensor_config = {'use_accelerometer': True, 'use_gyroscope': True}\n",
    "from lib.utils import load_data, resample\n",
    "\n",
    "for index,session in enumerate(val_sessions):\n",
    "    session_name = session['session_name']\n",
    "    raw_dataset_path = session['raw_dataset_path']\n",
    "    start_ns = session.get('start_ns')\n",
    "    stop_ns = session.get('stop_ns')\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    sensor_columns = []\n",
    "    if sensor_config.get('use_accelerometer', True):\n",
    "        sensor_columns.extend(['accel_x', 'accel_y', 'accel_z'])\n",
    "    if sensor_config.get('use_gyroscope', False):\n",
    "        sensor_columns.extend(['gyro_x', 'gyro_y', 'gyro_z'])\n",
    "\n",
    "    bouts = [b for b in session['bouts'] if b['label'] == labeling]\n",
    "    df = load_data(raw_dataset_path, session_name, sensor_config, start_ns, stop_ns)\n",
    "    df = resample(df)\n",
    "    df['label'] = 0\n",
    "\n",
    "    for bout in bouts:\n",
    "        start = bout['start']\n",
    "        end = bout['end']\n",
    "        df.loc[(df['ns_since_reboot'] >= start) & (df['ns_since_reboot'] <= end), 'label'] = 1\n",
    "\n",
    "    if 'accel_x' not in df.columns and sensor_config.get('use_accelerometer', True):\n",
    "        if 'x' in df.columns:\n",
    "            df.rename(columns={'x': 'accel_x', 'y': 'accel_y', 'z': 'accel_z'}, inplace=True)\n",
    "    data_columns = sensor_columns + ['label']\n",
    "    missing_columns = [col for col in data_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Warning: Missing columns {missing_columns} in session {session_name}\")\n",
    "        raise ValueError(f\"Missing columns {missing_columns} in session {session_name}\")\n",
    "\n",
    "    data = torch.tensor(df[data_columns].values, dtype=torch.float32)\n",
    "    if data.shape[0] < window_size:\n",
    "        padding_length = window_size - data.shape[0]\n",
    "        padding = torch.zeros((padding_length, data.shape[1]), dtype=torch.float32)\n",
    "        data = torch.cat([data, padding], dim=0)\n",
    "        print(f\"Zero-padded session {session_name} from {data.shape[0] - padding_length} to {data.shape[0]} samples\")\n",
    "\n",
    "    windowed_data = data.unfold(dimension=0,size=window_size,step=window_stride_eval)\n",
    "    X.append(windowed_data[:,:-1,:])\n",
    "    y.append(windowed_data[:,-1,:])\n",
    "\n",
    "    X = torch.cat(X)\n",
    "    y = (~(torch.cat(y) == 0).all(axis=1)).float()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X.to('cuda'))\n",
    "        predictions = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    num_windows = predictions.shape[0]  # 907\n",
    "\n",
    "    # Initialize output\n",
    "    time_domain_length = (num_windows - 1) * window_stride_eval + window_size\n",
    "    time_domain_preds = torch.zeros(time_domain_length)\n",
    "    overlap_counts = torch.zeros(time_domain_length)\n",
    "\n",
    "    # Accumulate predictions\n",
    "    for i, pred in enumerate(predictions):\n",
    "        start_idx = i * window_stride_eval\n",
    "        end_idx = start_idx + window_size\n",
    "        time_domain_preds[start_idx:end_idx] += pred.item()\n",
    "        overlap_counts[start_idx:end_idx] += 1\n",
    "\n",
    "    # Average where overlapping\n",
    "    time_domain_preds = time_domain_preds / overlap_counts\n",
    "\n",
    "    df = df.iloc[:len(time_domain_preds)]\n",
    "    df['logits'] = time_domain_preds\n",
    "    df['y_pred'] = (time_domain_preds > .5).int()\n",
    "\n",
    "    from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "    y_true = df['label'].values\n",
    "    y_pred = df['y_pred'].astype(int).values\n",
    "    # ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    # print(classification_report(y_true, y_pred))\n",
    "    print(index,session_name,f1_score(y_true, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48137c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one of x_train sessions in time domain with model predictions\n",
    "session = val_sessions[0]\n",
    "window_size = 3000\n",
    "window_stride_eval = 3000\n",
    "sensor_config = {'use_accelerometer': True, 'use_gyroscope': True}\n",
    "\n",
    "session_name = session['session_name']\n",
    "raw_dataset_path = session['raw_dataset_path']\n",
    "start_ns = session.get('start_ns')\n",
    "stop_ns = session.get('stop_ns')\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Determine which columns to use based on sensor config\n",
    "sensor_columns = []\n",
    "if sensor_config.get('use_accelerometer', True):\n",
    "    sensor_columns.extend(['accel_x', 'accel_y', 'accel_z'])\n",
    "if sensor_config.get('use_gyroscope', False):\n",
    "    sensor_columns.extend(['gyro_x', 'gyro_y', 'gyro_z'])\n",
    "\n",
    "bouts = [b for b in session['bouts'] if b['label'] == labeling]\n",
    "\n",
    "from lib.utils import load_data, resample\n",
    "\n",
    "df = load_data(raw_dataset_path, session_name, sensor_config, start_ns, stop_ns)\n",
    "df = resample(df)\n",
    "df['label'] = 0\n",
    "\n",
    "for bout in bouts:\n",
    "    start = bout['start']\n",
    "    end = bout['end']\n",
    "    df.loc[(df['ns_since_reboot'] >= start) & (df['ns_since_reboot'] <= end), 'label'] = 10\n",
    "\n",
    "if 'accel_x' not in df.columns and sensor_config.get('use_accelerometer', True):\n",
    "    if 'x' in df.columns:\n",
    "        df.rename(columns={'x': 'accel_x', 'y': 'accel_y', 'z': 'accel_z'}, inplace=True)\n",
    "data_columns = sensor_columns + ['label']\n",
    "missing_columns = [col for col in data_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warning: Missing columns {missing_columns} in session {session_name}\")\n",
    "    raise ValueError(f\"Missing columns {missing_columns} in session {session_name}\")\n",
    "\n",
    "data = torch.tensor(df[data_columns].values, dtype=torch.float32)\n",
    "\n",
    "if data.shape[0] < window_size:\n",
    "    # Zero pad the data to window size\n",
    "    padding_length = window_size - data.shape[0]\n",
    "    padding = torch.zeros((padding_length, data.shape[1]), dtype=torch.float32)\n",
    "    data = torch.cat([data, padding], dim=0)\n",
    "    print(f\"Zero-padded session {session_name} from {data.shape[0] - padding_length} to {data.shape[0]} samples\")\n",
    "\n",
    "windowed_data = data.unfold(dimension=0,size=window_size,step=window_stride_eval)\n",
    "X.append(windowed_data[:,:-1,:])\n",
    "y.append(windowed_data[:,-1,:])\n",
    "\n",
    "X = torch.cat(X)\n",
    "y = (~(torch.cat(y) == 0).all(axis=1)).float()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X.to('cuda'))\n",
    "    predictions = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "num_windows = predictions.shape[0]  # 907\n",
    "time_domain_length = (num_windows - 1) * window_stride_eval + window_size  # 909000\n",
    "time_domain_preds = torch.zeros(time_domain_length)\n",
    "overlap_counts = torch.zeros(time_domain_length)\n",
    "for i, pred in enumerate(predictions):\n",
    "    start_idx = i * window_stride_eval\n",
    "    end_idx = start_idx + window_size\n",
    "    time_domain_preds[start_idx:end_idx] += pred.item()\n",
    "    overlap_counts[start_idx:end_idx] += 1\n",
    "time_domain_preds = time_domain_preds / overlap_counts\n",
    "df = df.iloc[:len(time_domain_preds)]\n",
    "df['logits'] = time_domain_preds*10\n",
    "df['y_pred'] = (time_domain_preds > .5).int()*10\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "y_true = df['label'].values\n",
    "y_pred = df['y_pred'].astype(int).values\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred))\n",
    "import plotly.express as px\n",
    "fig = px.line(df.iloc[::30], x='ns_since_reboot', y=['accel_x','accel_y','accel_z','label','y_pred','logits'], title=f'Session {session_name} Smoking Prediction')\n",
    "fig.show(renderer='browser')\n",
    "session['raw_dataset_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc796b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee79d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
