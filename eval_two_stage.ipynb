{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block plots performance from entire experiments directory for something like a hyperparameter sweep.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from lib.models import TestModel\n",
    "from lib.train_utils import compute_loss_and_f1,evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data = []\n",
    "\n",
    "experiments_dir = f'./experiments'\n",
    "device = 'cuda'\n",
    "\n",
    "for experiment in tqdm(os.listdir(experiments_dir)):\n",
    "    for run in os.listdir(f'{experiments_dir}/{experiment}'):\n",
    "        if not os.path.exists(f'{experiments_dir}/{experiment}/{run}/metrics.json'):\n",
    "            print(f'Skipping {experiments_dir}/{experiment}/{run} as no metrics.json')\n",
    "            continue\n",
    "        \n",
    "        metrics = json.load(open(f'{experiments_dir}/{experiment}/{run}/metrics.json'))\n",
    "        losses = json.load(open(f'{experiments_dir}/{experiment}/{run}/losses.json'))\n",
    "        hyperparameters = json.load(open(f'{experiments_dir}/{experiment}/{run}/hyperparameters.json'))\n",
    "\n",
    "        if experiment.startswith('base'):\n",
    "            hyperparameters['mode'] = 'base'\n",
    "            best_base_model_path = f'{experiments_dir}/{experiment}/{run}/best_base_model.pt'\n",
    "            from lib.models import TestModel\n",
    "            target_participant = hyperparameters['target_participant']\n",
    "            data_path = hyperparameters['data_path']\n",
    "            batch_size = hyperparameters['batch_size']\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            target_test_dataset = TensorDataset(*torch.load(f'{data_path}/{target_participant}_test.pt'))\n",
    "            target_testloader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "            model = TestModel()\n",
    "            model.load_state_dict(torch.load(best_base_model_path, map_location='cpu'))\n",
    "            model.to(device)\n",
    "            test_loss, test_f1 = compute_loss_and_f1(model, target_testloader, criterion, device=device)\n",
    "            metrics['test_f1'] = test_f1\n",
    "            metrics['test_loss'] = test_loss\n",
    "        \n",
    "        # TODO: compute absolute improvement for each finetune model, may need two consecutive loops\n",
    "\n",
    "        data.append({\n",
    "            **metrics,\n",
    "            **hyperparameters,\n",
    "            'experiment': experiment,\n",
    "            'run': run\n",
    "        })\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "def get_base_test_f1(row):\n",
    "    if row['mode'] == 'base':\n",
    "        return row['test_f1']\n",
    "    else:\n",
    "        base_row = df[df['experiment'] == row['base_experiment_prefix']]\n",
    "        return base_row['test_f1'].iloc[0] if len(base_row) > 0 else None\n",
    "\n",
    "df['base_test_f1'] = df.apply(get_base_test_f1, axis=1)\n",
    "df['absolute_improvement'] = df['test_f1'] - df['base_test_f1']\n",
    "\n",
    "combo_df = df.groupby(['mode', 'target_data_pct', 'batch_size', 'fold']).size().reset_index(name='count')\n",
    "pivot_df = combo_df.pivot_table(values='count', \n",
    "                                index=['mode', 'target_data_pct'], \n",
    "                                columns=['fold'], \n",
    "                                fill_value=0)\n",
    "\n",
    "sns.heatmap(pivot_df, annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1ce267",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(21, 8), dpi=300)\n",
    "\n",
    "sns.boxplot(data=df, x='mode', y='test_f1', ax=ax[0,0])\n",
    "sns.boxplot(data=df, x='target_data_pct', y='test_f1', ax=ax[0,1])\n",
    "sns.boxplot(data=df, x='fold', y='test_f1', ax=ax[0,2])\n",
    "\n",
    "sns.boxplot(data=df, x='mode', y='absolute_improvement', ax=ax[1,0])\n",
    "sns.boxplot(data=df, x='target_data_pct', y='absolute_improvement', ax=ax[1,1])\n",
    "sns.boxplot(data=df, x='fold', y='absolute_improvement', ax=ax[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aae07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_test_f1(row):\n",
    "    if row['mode'] == 'base':\n",
    "        return row['test_f1']\n",
    "    else:\n",
    "        base_row = df[df['experiment'] == row['base_experiment_prefix']]\n",
    "        return base_row['test_f1'].iloc[0] if len(base_row) > 0 else None\n",
    "\n",
    "df['base_test_f1'] = df.apply(get_base_test_f1, axis=1)\n",
    "df['absolute_improvement'] = df['test_f1'] - df['base_test_f1']\n",
    "\n",
    "# Do similar for difference between target only fine tuning and full fine tuning paired by base experiment\n",
    "# Create a function to get the performance for each fine-tuning mode\n",
    "def get_finetune_comparison(df):\n",
    "    # Filter to only fine-tuning experiments (not base)\n",
    "    finetune_df = df[df['mode'].isin(['target_only_fine_tuning', 'full_fine_tuning'])].copy()\n",
    "    \n",
    "    # Pivot to get both modes side by side for each base experiment\n",
    "    comparison_df = finetune_df.pivot_table(\n",
    "        values='test_f1', \n",
    "        index=['base_experiment_prefix', 'fold'], \n",
    "        columns='mode', \n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate the difference (full - target_only)\n",
    "    comparison_df['full_vs_target_only_diff'] = (\n",
    "        comparison_df['full_fine_tuning'] - comparison_df['target_only_fine_tuning']\n",
    "    )\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Get the comparison\n",
    "finetune_comparison = get_finetune_comparison(df)\n",
    "\n",
    "# Add this back to the main dataframe\n",
    "df = df.merge(\n",
    "    finetune_comparison[['base_experiment_prefix', 'fold', 'full_vs_target_only_diff']], \n",
    "    on=['base_experiment_prefix', 'fold'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Now you can visualize which approach is better\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Box plot of the difference\n",
    "sns.boxplot(data=finetune_comparison, y='full_vs_target_only_diff', ax=axes[0])\n",
    "axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[0].set_title('Full Fine-tuning vs Target-only\\n(Positive = Full Better)')\n",
    "axes[0].set_ylabel('Performance Difference')\n",
    "\n",
    "# Side-by-side comparison\n",
    "finetune_melted = finetune_comparison.melt(\n",
    "    id_vars=['base_experiment_prefix', 'fold'],\n",
    "    value_vars=['target_only_fine_tuning', 'full_fine_tuning'],\n",
    "    var_name='fine_tuning_mode',\n",
    "    value_name='test_f1'\n",
    ")\n",
    "sns.boxplot(data=finetune_melted, x='fine_tuning_mode', y='test_f1', ax=axes[1])\n",
    "axes[1].set_title('Performance by Fine-tuning Mode')\n",
    "\n",
    "# Scatter plot showing paired comparison\n",
    "sns.scatterplot(\n",
    "    data=finetune_comparison, \n",
    "    x='target_only_fine_tuning', \n",
    "    y='full_fine_tuning',\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].plot([0, 1], [0, 1], 'r--', alpha=0.7)  # Diagonal line\n",
    "axes[2].set_title('Paired Comparison\\n(Above line = Full Better)')\n",
    "axes[2].set_xlabel('Target-only Fine-tuning F1')\n",
    "axes[2].set_ylabel('Full Fine-tuning F1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary of Fine-tuning Comparison:\")\n",
    "print(f\"Mean difference (Full - Target-only): {finetune_comparison['full_vs_target_only_diff'].mean():.4f}\")\n",
    "print(f\"Std of difference: {finetune_comparison['full_vs_target_only_diff'].std():.4f}\")\n",
    "print(f\"Full fine-tuning better in {(finetune_comparison['full_vs_target_only_diff'] > 0).sum()} out of {len(finetune_comparison)} cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd70731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "df = df[df['mode'] != 'base']  # Exclude base for this plot because it has no target_data_pct\n",
    "sns.boxplot(data=df, x='mode', y='test_f1', hue='target_data_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b62efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "        metrics = json.load(open(f'{experiments_dir}/{experiment}/{run}/metrics.json'))\n",
    "        losses = json.load(open(f'{experiments_dir}/{experiment}/{run}/losses.json'))\n",
    "        hyperparameters = json.load(open(f'{experiments_dir}/{experiment}/{run}/hyperparameters.json'))\n",
    "\n",
    "        data.append({\n",
    "            'hyperparameter_hash': experiment.split('_')[-1],\n",
    "            'batch_size': hyperparameters['batch_size'], \n",
    "            'fold': int(run.split('_')[0].replace('fold','')), \n",
    "            'target_data_pct': float(hyperparameters['target_data_pct']), \n",
    "            'n_base_participants': int(hyperparameters['n_base_participants']),\n",
    "            'mode': hyperparameters['mode'],\n",
    "            'best_target_model_target_val_f1': metrics['best_target_model_target_val_f1'], \n",
    "            'best_target_model_target_test_f1': metrics['best_target_model_target_test_f1'],\n",
    "            'best_base_model_target_val_f1': metrics['best_base_model_target_val_f1'] if 'best_base_model_target_val_f1' in metrics else None,\n",
    "            'best_base_model_target_test_f1': metrics['best_base_model_target_test_f1'] if 'best_base_model_target_test_f1' in metrics else None,\n",
    "            'best_target_model_target_val_loss': metrics['best_target_model_target_val_loss'] if 'best_target_model_target_val_loss' in metrics else None,\n",
    "            'best_target_model_target_test_loss': metrics['best_target_model_target_test_loss'] if 'best_target_model_target_test_loss' in metrics else None,\n",
    "            'best_base_model_target_val_loss': metrics['best_base_model_target_val_loss'] if 'best_base_model_target_val_loss' in metrics else None,\n",
    "            'best_base_model_target_test_loss': metrics['best_base_model_target_test_loss'] if 'best_base_model_target_test_loss' in metrics else None,\n",
    "            'absolute_improvement_target_test_f1': (metrics['best_target_model_target_test_f1'] - metrics['best_base_model_target_test_f1']) if 'best_base_model_target_test_f1' in metrics else None,\n",
    "            'absolute_improvement_target_val_f1': (metrics['best_target_model_target_val_f1'] - metrics['best_base_model_target_val_f1']) if 'best_base_model_target_val_f1' in metrics else None,\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab702da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f722933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['hyperparameter_hash'].value_counts())\n",
    "print(df['mode'].value_counts())\n",
    "print(df['target_data_pct'].value_counts())\n",
    "print(df['batch_size'].value_counts())\n",
    "print(df['n_base_participants'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "sns.boxplot(data=df_plot, x='fold', y='best_target_model_target_val_f1', hue='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf792ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "# df_plot = df_plot[df_plot['mode'] == 'full_fine_tuning']\n",
    "df_plot = df_plot[df_plot['target_data_pct'] == 1]\n",
    "# df_plot = df_plot[df_plot['n_base_participants'] == 6]\n",
    "df_plot = df_plot.melt(value_vars=['best_base_model_target_test_f1','best_target_model_target_test_f1'],id_vars=['fold','batch_size','target_data_pct','n_base_participants','mode'], var_name='metric', value_name='value')\n",
    "sns.boxplot(data=df_plot, x='metric', y='value', hue='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b021b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "df_plot = df_plot[df_plot['mode'] == 'target_only_fine_tuning']\n",
    "# df_plot = df_plot[df_plot['target_data_pct'] == 1]\n",
    "# df_plot = df_plot[df_plot['n_base_participants'] == 6]\n",
    "df_plot = df_plot.melt(value_vars=['best_base_model_target_test_f1','best_target_model_target_test_f1'],id_vars=['fold','batch_size','target_data_pct','n_base_participants','mode'], var_name='metric', value_name='value')\n",
    "df_plot\n",
    "sns.boxplot(data=df_plot, x='fold', y='value', hue='metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05ccf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc11b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='mode', y='best_target_model_target_test_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7beb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='mode', y='best_target_model_target_test_f1', hue='target_data_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d77fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,2, figsize=(12,10))\n",
    "sns.boxplot(data=df, x='target_data_pct', y='best_target_model_target_val_loss', ax=ax[0,0])\n",
    "ax[0,0].set_yscale('log')\n",
    "sns.boxplot(data=df, x='target_data_pct', y='best_target_model_target_val_f1', ax=ax[1,0])\n",
    "sns.boxplot(data=df, x='batch_size', y='best_target_model_target_val_f1', hue='target_data_pct', ax=ax[0,1])\n",
    "sns.boxplot(data=df, x='batch_size', y='best_target_model_target_val_loss', hue='target_data_pct', ax=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "df_plot = df_plot[df_plot['mode'] == 'target_only_fine_tuning']\n",
    "# df_plot = df_plot[df_plot['target_data_pct'] == 1]\n",
    "sns.boxplot(data=df_plot, x='n_base_participants', y='best_target_model_target_test_f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69987b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_hash = df.groupby('hyperparameter_hash')['best_target_model_target_val_f1'].mean().sort_values(ascending=False).keys()[0]\n",
    "print(best_model_hash)\n",
    "df_metrics = df[df['hyperparameter_hash'] == best_model_hash].copy()\n",
    "# Compute Metrics For Paper\n",
    "display(df_metrics)\n",
    "df_metrics = df_metrics[['fold','best_base_model_target_test_f1','best_target_model_target_test_f1']]\n",
    "df_metrics.sort_values('fold', inplace=True)\n",
    "# Add Absolute Improvement\n",
    "df_metrics['absolute_improvement'] = df_metrics['best_target_model_target_test_f1'] - df_metrics['best_base_model_target_test_f1']\n",
    "# Add Relative Improvement\n",
    "df_metrics['relative_improvement'] = df_metrics['absolute_improvement'] / df_metrics['best_base_model_target_test_f1']\n",
    "# Add Room For Improvement Metric\n",
    "df_metrics['room_for_improvement'] = (1 - df_metrics['best_base_model_target_test_f1'])\n",
    "df_metrics['room_for_improvement_pct'] = df_metrics['absolute_improvement'] / df_metrics['room_for_improvement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51943e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models import TestModel\n",
    "from lib.train_utils import compute_loss_and_f1,evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from lib.train_utils import random_subsample\n",
    "device = 'cpu'\n",
    "\n",
    "# Load Model From Best Model Hash\n",
    "# You can change this to load a different model if you want to inspect it\n",
    "best_model_dir = [d for d in os.listdir(experiments_dir) if best_model_hash in d][0]\n",
    "print(f'Loading model from {best_model_dir}')\n",
    "\n",
    "run_dirs = os.listdir(f'{experiments_dir}/{best_model_dir}')\n",
    "print(f'Runs: {run_dirs}')\n",
    "\n",
    "df_metrics['best_target_model_target_test_precision'] = 0.0\n",
    "df_metrics['best_target_model_target_test_recall'] = 0.0\n",
    "\n",
    "for run_dir in run_dirs:\n",
    "    print(f'Loading model from {run_dir}')\n",
    "    best_base_model_path = f'{experiments_dir}/{best_model_dir}/{run_dir}/best_base_model.pt'\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    hyperparameters = json.load(open(f'{experiments_dir}/{best_model_dir}/{run_dir}/hyperparameters.json'))\n",
    "    target_participant = hyperparameters['target_participant']\n",
    "    data_path = hyperparameters['data_path']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    fold = hyperparameters['fold']\n",
    "    print(f'Loading data from {data_path} for target participant {target_participant}')\n",
    "    target_train_dataset = TensorDataset(*torch.load(f'{data_path}/{target_participant}_train.pt'))\n",
    "    target_val_dataset = TensorDataset(*torch.load(f'{data_path}/{target_participant}_val.pt'))\n",
    "    target_test_dataset = TensorDataset(*torch.load(f'{data_path}/{target_participant}_test.pt'))\n",
    "\n",
    "    # Subsample target training data if specified\n",
    "    target_data_pct = hyperparameters['target_data_pct']\n",
    "    if target_data_pct < 1.0:\n",
    "        print(f'Target train dataset size: {len(target_train_dataset)}')\n",
    "        target_train_dataset = random_subsample(target_train_dataset, target_data_pct)\n",
    "        print(f'Target train dataset size: {len(target_train_dataset)}')\n",
    "\n",
    "    print(f'Target val dataset size: {len(target_val_dataset)}')\n",
    "    target_val_dataset = random_subsample(target_val_dataset, .1)\n",
    "    print(f'Target val dataset size: {len(target_val_dataset)}')\n",
    "\n",
    "    target_trainloader = DataLoader(target_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    target_valloader = DataLoader(target_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    target_testloader = DataLoader(target_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TestModel()\n",
    "    print(f'Loading base model from {best_base_model_path}')\n",
    "    model.load_state_dict(torch.load(best_base_model_path, map_location='cpu'))\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss, test_f1 = compute_loss_and_f1(model, target_testloader, criterion, device=device)\n",
    "    y_true,y_pred = evaluate(model, target_testloader, device=device)\n",
    "\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "\n",
    "    df_metrics.loc[df_metrics['fold'] == fold,'best_target_model_target_test_precision'] = precision\n",
    "    df_metrics.loc[df_metrics['fold'] == fold,'best_target_model_target_test_recall'] = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4a77a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target val f1 learning curves\n",
    "curve_data = []\n",
    "for experiment in tqdm(os.listdir(experiments_dir)):\n",
    "    for run in os.listdir(f'{experiments_dir}/{experiment}'):\n",
    "        if not os.path.exists(f'{experiments_dir}/{experiment}/{run}/losses.json'):\n",
    "            continue\n",
    "        losses = json.load(open(f'{experiments_dir}/{experiment}/{run}/losses.json'))\n",
    "        hyperparameters = json.load(open(f'{experiments_dir}/{experiment}/{run}/hyperparameters.json'))\n",
    "\n",
    "        if hyperparameters['mode'] != 'full_fine_tuning':\n",
    "            continue\n",
    "        \n",
    "        for epoch, f1 in enumerate(losses['target val f1']):\n",
    "            curve_data.append({\n",
    "                'epoch': epoch,\n",
    "                'target_val_f1': f1,\n",
    "                'fold': run\n",
    "            })\n",
    "\n",
    "curve_df = pd.DataFrame(curve_data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=curve_df, x='epoch', y='target_val_f1', hue='fold')\n",
    "plt.title('Target Validation F1 Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Target Val F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e6132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
