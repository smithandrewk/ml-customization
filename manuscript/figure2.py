import os
import json
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy import stats
import argparse

# Configure matplotlib for high-quality vector graphics
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend

# Parse command line arguments
parser = argparse.ArgumentParser(description='Generate Figure 2: Main Results - Performance Improvements')
parser.add_argument('--experiment', type=str, help='Specific experiment directory to plot (e.g., alpha). If not specified, plots all experiments separately.')
args = parser.parse_args()

# Set Nature-style publication parameters
plt.rcParams.update({
    'font.size': 9,
    'axes.labelsize': 10,
    'axes.titlesize': 11,
    'xtick.labelsize': 8,
    'ytick.labelsize': 8,
    'legend.fontsize': 8,
    'figure.titlesize': 12,
    'axes.linewidth': 0.8,
    'xtick.major.width': 0.8,
    'ytick.major.width': 0.8,
    'xtick.minor.width': 0.6,
    'ytick.minor.width': 0.6,
    'axes.spines.top': False,
    'axes.spines.right': False,
    'figure.facecolor': 'white'
})

# Nature-inspired color palette
colors = {
    'base': '#1f77b4',      # Professional blue
    'target': '#d62728',    # Nature red
    'improve': '#2ca02c',   # Nature green
    'neutral': '#7f7f7f',   # Gray
    'accent': '#ff7f0e'     # Orange accent
}

# Load experiment data
if args.experiment:
    if os.path.isdir(f'experiments/{args.experiment}'):
        experiments = [args.experiment]
        print(f"Plotting specific experiment: {args.experiment}")
    else:
        print(f"Error: Experiment directory 'experiments/{args.experiment}' not found")
        if os.path.exists('experiments'):
            print(f"Available experiments: {', '.join(os.listdir('experiments'))}")
        exit(1)
else:
    if os.path.exists('experiments'):
        experiments = [d for d in os.listdir('experiments') if os.path.isdir(f'experiments/{d}')]
        print(f"Plotting each experiment separately: {experiments}")
    else:
        print("No experiments directory found")
        exit(1)

# Process each experiment separately
for experiment in experiments:
    print(f'\\n=== Processing experiment: {experiment} ===')
    data = []
    experiment_dir = f'experiments/{experiment}'

    if not os.path.isdir(experiment_dir):
        continue

    # Get all training run subdirectories
    training_runs = [d for d in os.listdir(experiment_dir)
                    if os.path.isdir(f'{experiment_dir}/{d}') and
                    os.path.exists(f'{experiment_dir}/{d}/hyperparameters.json')]

    if not training_runs:
        print(f"  No valid training runs found in {experiment}")
        continue

    print(f"  Found {len(training_runs)} training runs: {training_runs}")

    # Load data from each training run
    for run_id in training_runs:
        run_dir = f'{experiment_dir}/{run_id}'

        # Check all required files exist
        required_files = ['hyperparameters.json', 'metrics.json', 'losses.json']
        if not all(os.path.exists(f'{run_dir}/{file}') for file in required_files):
            print(f"    Skipping {run_id}: missing required files")
            continue

        try:
            with open(f'{run_dir}/hyperparameters.json', 'r') as f:
                hyperparameters = json.load(f)

            with open(f'{run_dir}/metrics.json', 'r') as f:
                metrics = json.load(f)

            with open(f'{run_dir}/losses.json', 'r') as f:
                losses = json.load(f)

            fold = hyperparameters['fold']
            participants = hyperparameters['participants']
            participants = ['alsaad','anam','asfik','ejaz','iftakhar','tonmoy','unk1','dennis']
            target_participant = participants[fold] if fold < len(participants) else f'fold_{fold}'

            # Use test set metrics for unbiased evaluation
            base_f1 = metrics.get('base_test_f1', metrics.get('best_target_val_f1_from_best_base_model', 0))
            target_f1 = metrics.get('test_f1', metrics.get('best_target_val_f1', 0))
            improvement = target_f1 - base_f1

            data.append({
                'experiment': f'{experiment}/{run_id}',
                'experiment_name': experiment,
                'run_id': run_id,
                'fold': fold,
                'participant': target_participant,
                'base_f1': base_f1,
                'target_f1': target_f1,
                'improvement': improvement,
                'relative_improvement': improvement / base_f1 * 100,
                'transition_epoch': metrics.get('transition_epoch', 0),
                'losses': losses
            })
            print(f"    Loaded run {run_id}: fold {fold} ({target_participant})")

        except Exception as e:
            print(f"    Error loading {run_id}: {e}")
            continue

    if not data:
        print(f"  No data loaded for experiment {experiment}, creating demo figure...")
        # Create demo data for visualization
        np.random.seed(42)
        data = []
        for i in range(8):
            base_f1 = np.random.uniform(0.75, 0.95)
            improvement = np.random.normal(0.04, 0.03)
            target_f1 = base_f1 + improvement
            data.append({
                'experiment': f'demo/fold{i}',
                'fold': i,
                'participant': f'P{i}',
                'base_f1': base_f1,
                'target_f1': target_f1,
                'improvement': improvement,
                'relative_improvement': improvement / base_f1 * 100,
                'transition_epoch': 50,
                'losses': {
                    'target val loss': list(np.random.exponential(0.5, 100)),
                    'target val f1': list(np.random.beta(2, 2, 100))
                }
            })
        print(f"  Created demo dataset with {len(data)} participants")

    print(f'  Loaded {len(data)} training runs for {experiment}')
    for d in data:
        print(f'  Fold {d["fold"]} ({d["participant"]}): Base F1: {d["base_f1"]:.4f}, Target F1: {d["target_f1"]:.4f}, Improvement: {d["improvement"]:.4f} ({d["relative_improvement"]:.1f}%)')

    # Create four-panel figure for main results
    fig, axes = plt.subplots(2, 2, figsize=(7.2, 6.8))  # Nature single column width
    fig.patch.set_facecolor('white')

    # Panel A: Box plots for F1 score comparison
    ax1 = axes[0, 0]
    base_f1s = [d['base_f1'] for d in data]
    target_f1s = [d['target_f1'] for d in data]

    box_data = [base_f1s, target_f1s]
    bp = ax1.boxplot(box_data, labels=['Base', 'Customized'], patch_artist=True,
                    widths=0.6, medianprops=dict(color='white', linewidth=1.5))
    bp['boxes'][0].set_facecolor(colors['base'])
    bp['boxes'][0].set_alpha(0.8)
    bp['boxes'][0].set_edgecolor(colors['base'])
    bp['boxes'][1].set_facecolor(colors['target'])
    bp['boxes'][1].set_alpha(0.8)
    bp['boxes'][1].set_edgecolor(colors['target'])

    # Style whiskers and caps
    for whisker in bp['whiskers']:
        whisker.set_color(colors['neutral'])
        whisker.set_linewidth(1)
    for cap in bp['caps']:
        cap.set_color(colors['neutral'])
        cap.set_linewidth(1)

    # Add individual data points
    x_base = np.random.normal(1, 0.04, len(base_f1s))
    x_target = np.random.normal(2, 0.04, len(target_f1s))
    ax1.scatter(x_base, base_f1s, alpha=0.7, s=25, color=colors['base'], edgecolors='white', linewidths=0.5)
    ax1.scatter(x_target, target_f1s, alpha=0.7, s=25, color=colors['target'], edgecolors='white', linewidths=0.5)

    # Add statistical test
    t_stat, p_value = stats.ttest_rel(target_f1s, base_f1s)
    if p_value < 0.001:
        p_text = 'P < 0.001'
    else:
        p_text = f'P = {p_value:.3f}'

    # Calculate Cohen's d for paired samples
    improvements = [target - base for target, base in zip(target_f1s, base_f1s)]
    cohens_d = np.mean(improvements) / np.std(improvements, ddof=1)

    ax1.text(0.98, 0.98, f'{p_text}\nCohen' + "'s d = " + f'{cohens_d:.2f}',
             transform=ax1.transAxes, ha='right', va='top',
             fontweight='bold', fontsize=8)

    ax1.set_ylabel('F1 score', fontweight='bold')
    ax1.set_title('a', fontweight='bold', fontsize=12, loc='left', pad=10)
    ax1.grid(True, alpha=0.2, linewidth=0.5)
    ax1.set_ylim(0.0, 1.0)

    # Panel B: Per-participant improvement bar chart
    ax2 = axes[0, 1]
    participant_labels = [f'P{d["fold"]}' for d in data]
    improvements = [d['improvement'] for d in data]

    # Color bars by improvement direction
    bar_colors = [colors['improve'] if imp > 0 else colors['target'] for imp in improvements]

    bars = ax2.bar(range(len(participant_labels)), improvements,
                   color=bar_colors, alpha=0.8, edgecolor='white', linewidth=0.5)

    # Add horizontal line at y=0
    ax2.axhline(y=0, color=colors['neutral'], linestyle='-', alpha=0.5, linewidth=1)

    # Add value labels on bars
    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + (0.005 if height > 0 else -0.01),
                f'{imp:.3f}', ha='center', va='bottom' if height > 0 else 'top', fontsize=7)

    ax2.set_xlabel('Participant', fontweight='bold')
    ax2.set_ylabel('Improvement (Î”F1)', fontweight='bold')
    ax2.set_title('b', fontweight='bold', fontsize=12, loc='left', pad=10)
    ax2.set_xticks(range(len(participant_labels)))
    ax2.set_xticklabels(participant_labels, fontsize=8)
    ax2.grid(True, alpha=0.2, axis='y', linewidth=0.5)

    # Panel C: Baseline performance vs improvement potential
    ax3 = axes[1, 0]

    # Color points based on improvement direction
    point_colors = [colors['improve'] if imp > 0 else colors['target'] for imp in improvements]

    scatter = ax3.scatter(base_f1s, improvements, c=point_colors, alpha=0.8, s=40,
                         edgecolors='white', linewidths=0.5)

    # Add correlation analysis
    correlation = np.corrcoef(base_f1s, improvements)[0, 1]
    from scipy.stats import pearsonr
    r, p_corr = pearsonr(base_f1s, improvements)

    # Add trend line if correlation is significant
    if p_corr < 0.05:
        z = np.polyfit(base_f1s, improvements, 1)
        p = np.poly1d(z)
        x_trend = np.linspace(min(base_f1s), max(base_f1s), 100)
        ax3.plot(x_trend, p(x_trend), color=colors['neutral'], linestyle='-', alpha=0.8, linewidth=1)

    # Add horizontal line at y=0
    ax3.axhline(y=0, color=colors['neutral'], linestyle='--', alpha=0.7, linewidth=1)

    # Add participant labels
    for i, d in enumerate(data):
        ax3.annotate(f'P{d["fold"]}', (base_f1s[i], improvements[i]),
                    xytext=(3, 3), textcoords='offset points', fontsize=6,
                    alpha=0.7, color=colors['neutral'])

    # Add correlation statistics
    if p_corr < 0.001:
        p_corr_text = 'P < 0.001'
    else:
        p_corr_text = f'P = {p_corr:.3f}'

    ax3.text(0.02, 0.98, f'r = {correlation:.3f}\n{p_corr_text}',
             transform=ax3.transAxes, ha='left', va='top', fontweight='bold', fontsize=8)

    ax3.set_xlabel('Baseline F1 score', fontweight='bold')
    ax3.set_ylabel('Improvement (Î”F1)', fontweight='bold')
    ax3.set_title('c', fontweight='bold', fontsize=12, loc='left', pad=10)
    ax3.grid(True, alpha=0.2, linewidth=0.5)

    # Panel D: Effect size analysis and success rate
    ax4 = axes[1, 1]

    # Create effect size visualization
    positive_improvements = [imp for imp in improvements if imp > 0]
    negative_improvements = [imp for imp in improvements if imp <= 0]

    # Success rate pie chart
    success_rate = len(positive_improvements) / len(improvements) * 100
    sizes = [success_rate, 100 - success_rate]
    labels = [f'Improved\n({len(positive_improvements)}/8)', f'No improvement\n({len(negative_improvements)}/8)']
    colors_pie = [colors['improve'], colors['neutral']]

    wedges, texts, autotexts = ax4.pie(sizes, labels=labels, colors=colors_pie, autopct='%1.1f%%',
                                      startangle=90, textprops={'fontsize': 8})

    # Add summary statistics
    mean_improvement = np.mean(improvements)
    median_improvement = np.median(improvements)

    ax4.text(0.02, -1.3, f'Mean Î”: {mean_improvement:.3f}\nMedian Î”: {median_improvement:.3f}\nCohen' + "'s d: " + f'{cohens_d:.2f}',
             transform=ax4.transAxes, ha='left', va='top', fontweight='bold', fontsize=8,
             bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor='gray'))

    ax4.set_title('d', fontweight='bold', fontsize=12, loc='left', pad=10)

    # Adjust layout
    plt.tight_layout(pad=1.0, h_pad=1.5, w_pad=1.5)

    # Print summary statistics
    print(f'\\\\n=== Summary Statistics for {experiment} ===')
    print(f'Mean base F1: {np.mean(base_f1s):.4f} Â± {np.std(base_f1s):.4f}')
    print(f'Mean customized F1: {np.mean(target_f1s):.4f} Â± {np.std(target_f1s):.4f}')
    print(f'Mean improvement: {np.mean(improvements):.4f} Â± {np.std(improvements):.4f}')
    print(f'Success rate: {success_rate:.1f}% ({len(positive_improvements)}/{len(improvements)})')
    print(f'Effect size (Cohen' + "'s d): " + f'{cohens_d:.3f}')
    print(f'Statistical significance: {p_text}')

    # Save figure with experiment-specific filename as PDF vector graphics
    os.makedirs('figures', exist_ok=True)
    filename = f'figures/figure2_{experiment}.pdf'
    plt.savefig(filename, bbox_inches='tight', format='pdf', dpi=300)
    print(f'\\\\nFigure saved as {filename}')

    # Always close figure after saving to avoid GUI popup and memory issues
    plt.close()

print("\\\\nProcessing complete!")