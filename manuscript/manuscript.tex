\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{cite}
\usepackage{url}
\usepackage[margin=1in]{geometry}

% Experiment configuration - CHANGE THIS TO YOUR EXPERIMENT NAME
\newcommand{\experimentname}{b256_aug_patience5_full_fine_tuning_20250929_112052}

% Include auto-generated statistics for abstract
\input{abstract_stats.tex}

\title{Personalized Neural Networks for Cigarette Smoking Detection Using Accelerometer Data}

\author{
% TODO: Add author names and affiliations
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper investigates the effectiveness of personalizing neural networks for cigarette smoking detection using accelerometer data from wearable devices. We demonstrate that fine-tuning a general smoking detection model on individual participants' data significantly improves performance compared to a one-size-fits-all approach. Using a leave-one-out cross-validation study across \numparticipants{} participants, we show that personalized models achieve substantial improvements in F1-score, with individual gains ranging from \mingain\% to \maxgain\% (mean: \meangain\%). Our findings suggest that personalization is a promising approach for improving the accuracy of behavioral detection systems in real-world deployments, with \responderrate\% of participants showing improved performance.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

% TODO: Complete introduction with proper motivation and background

Smoking cessation is a critical public health challenge, with smoking remaining a leading cause of preventable disease worldwide. Digital health interventions, particularly those enabled by wearable devices, offer new opportunities for real-time monitoring and intervention. Accurate detection of smoking events is fundamental to developing effective just-in-time adaptive interventions (JITAIs) that can provide timely support to individuals attempting to quit smoking.

Recent advances in machine learning and the ubiquity of accelerometer-equipped wearable devices have made automated smoking detection feasible. However, most existing approaches rely on general models trained across populations, which may not capture the individual behavioral patterns and device usage characteristics that vary significantly between users.

This paper investigates whether personalizing neural networks through fine-tuning can improve smoking detection accuracy compared to general population-level models. We hypothesize that individual behavioral signatures in accelerometer data can be better captured through personalized models, leading to improved detection performance.

\section{Methods}
\label{sec:methods}

% TODO: Complete methods section with detailed methodology

\subsection{Dataset and Participants}

Our study utilized accelerometer data collected from 8 participants using wearable devices. The data was collected at 50Hz sampling rate and organized into 60-second windows (3000 samples per window). Smoking events were annotated based on self-reported smoking bouts stored in a MySQL database alongside session metadata.

\subsection{Model Architecture}

We employed a lightweight 1D convolutional neural network optimized for accelerometer time-series classification. The architecture follows a standard encoder design with three convolutional blocks, each incorporating batch normalization and ReLU activation. Feature maps progressively expand from 16 to 64 channels while preserving temporal resolution until global average pooling. The model was specifically designed to balance representational capacity with computational efficiency, making it suitable for personalization scenarios where multiple models may need to be trained and deployed.

The network processes 3000-sample input windows corresponding to 60 seconds of accelerometer data at 50Hz sampling rate, and outputs binary smoking probability predictions. Complete architectural details are provided in Appendix~\ref{appendix:architecture}.

\subsection{Transfer Learning Strategies}

To systematically evaluate personalization approaches, we compared three transfer learning strategies:

\textbf{1. Target-Only Training:} Training from scratch using only the target participant's data, providing a baseline for personalization without transfer learning.

\textbf{2. Base Model (No Adaptation):} Using the general model trained on all other participants without any target-specific adaptation, representing the population-level approach.

\textbf{3. Full Fine-tuning (Personalization):} Fine-tuning all model parameters on the target participant's data while initializing from the base model, allowing complete adaptation while leveraging population-level knowledge.

\subsection{Training Methodology}

Our training protocol consisted of two phases:

\textbf{Phase 1: Base Model Training}
A general model was trained on data from all participants except the target participant (leave-one-out approach). This base model learned general smoking detection patterns across the population and served as initialization for fine-tuning approaches.

\textbf{Phase 2: Target Adaptation}
Depending on the transfer learning strategy, we either: (1) trained a new model from scratch on target data, (2) used the base model directly, or (3) fine-tuned all parameters on the target participant's data.

\subsection{Evaluation}

We conducted leave-one-out cross-validation across all 8 participants. For each participant, we evaluated all three transfer learning strategies:
\begin{enumerate}
    \item Trained a base model on the remaining 7 participants
    \item Applied each transfer learning strategy using the target participant's training data
    \item Evaluated all approaches on the target participant's validation data
    \item Measured computational efficiency metrics for each strategy
\end{enumerate}

Performance was measured using F1-score and validation loss as primary metrics. Additionally, we tracked computational efficiency metrics including training time, epochs to convergence, and GPU memory usage to assess the practical trade-offs between performance gains and computational overhead.

\section{Results}
\label{sec:results}

Our leave-one-out cross-validation results demonstrate consistent improvements from personalization across participants.

% Auto-generated results - to regenerate, run: python generate_results.py <experiment_name>
\input{results.tex}

\section{Discussion}
\label{sec:discussion}

% TODO: Complete discussion section

Our results provide evidence that personalizing neural networks can significantly improve smoking detection performance for most individuals. The mean improvement of 9.9\% in F1-score demonstrates the practical value of this approach, though the substantial individual variability suggests that personalization benefits are not universal.

\subsection{Implications for Practice}

The finding that 6 out of 8 participants (75\%) benefited from personalization suggests that adaptive systems could implement personalization selectively, potentially using validation performance to determine when personalization is beneficial.

\subsection{Limitations}

Several limitations should be considered:
\begin{itemize}
    \item Small sample size (8 participants) limits generalizability
    \item Two participants showed decreased performance, suggesting potential overfitting or insufficient data
    \item The study did not investigate optimal fine-tuning strategies or hyperparameters
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

% TODO: Complete conclusion

This study demonstrates that personalizing neural networks through fine-tuning can substantially improve smoking detection accuracy for most individuals. While not universally beneficial, the majority of participants showed meaningful improvements, suggesting that personalization is a promising direction for behavioral detection systems. Future work should investigate methods to predict which individuals will benefit from personalization and optimize fine-tuning strategies for better consistency across users.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{Model Architecture Details}
\label{appendix:architecture}

\begin{table}[H]
\centering
\caption{SimpleSmokingCNN Architecture Specification}
\label{tab:architecture}
\begin{tabular}{@{}lllll@{}}
\toprule
Layer & Type & Input Shape & Output Shape & Parameters \\
\midrule
Input & - & (batch, 3, 3000) & (batch, 3, 3000) & - \\
Conv1D-1 & Conv1D + BN + ReLU & (batch, 3, 3000) & (batch, 16, 3000) & kernel=5, padding=2 \\
Conv1D-2 & Conv1D + BN + ReLU & (batch, 16, 3000) & (batch, 32, 3000) & kernel=5, padding=2 \\
Conv1D-3 & Conv1D + BN + ReLU & (batch, 32, 3000) & (batch, 64, 3000) & kernel=5, padding=2 \\
GlobalAvgPool & AdaptiveAvgPool1d & (batch, 64, 3000) & (batch, 64, 1) & pool\_size=1 \\
Flatten & Squeeze & (batch, 64, 1) & (batch, 64) & - \\
Dropout & Dropout & (batch, 64) & (batch, 64) & p=0.3 \\
Classifier & Linear & (batch, 64) & (batch, 1) & out\_features=1 \\
\bottomrule
\end{tabular}
\end{table}

\noindent \textbf{Activation Functions:} ReLU activation is applied after each convolutional layer following batch normalization. The final output uses sigmoid activation for binary classification.

\noindent \textbf{Regularization:} Batch normalization is applied after each convolutional layer, and dropout (p=0.3) is applied before the final classifier to prevent overfitting.

\noindent \textbf{Total Parameters:} Approximately 4,100 trainable parameters, making the model lightweight and suitable for personalization scenarios.

\end{document}