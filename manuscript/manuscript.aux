\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{who2021tobacco}
\citation{nahum2018just}
\citation{sazonov2013smoke,saleheen2015puffmarker}
\citation{tang2020individualized}
\citation{stone2007patient}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Results}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dataset and Experimental Design}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Population Models Exhibit Poor Generalization to Individuals}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Fine-Tuning Achieves Robust Personalization with Full Target Data}{2}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Transfer Learning Excels in Low-Data Regimes}{3}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Fine-Tuning Outperforms Training From Scratch}{3}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Ablation: Base Model Size Requirements}{3}{subsection.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Performance metrics by participant (100\% target data, N=6 base participants).} All metrics computed on held-out test sets. Fold numbers correspond to leave-one-participant-out cross-validation folds. Negative absolute improvement for folds 4-5 reflects negligible degradation within measurement noise.}}{4}{table.1}\protected@file@percent }
\newlabel{tab:performance}{{1}{4}{\textbf {Performance metrics by participant (100\% target data, N=6 base participants).} All metrics computed on held-out test sets. Fold numbers correspond to leave-one-participant-out cross-validation folds. Negative absolute improvement for folds 4-5 reflects negligible degradation within measurement noise}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Transfer learning enables robust personalization across participants.} \textbf  {(A)} Population-trained base models show variable performance across participants (blue, median F1 = 0.87), while fine-tuned models achieve consistent high performance (orange, median F1 = 0.91). Boxplots show distribution across hyperparameter configurations; individual points represent specific experimental runs. \textbf  {(B)} Performance improvement is consistent across all seven participants (folds 0-6). Each fold represents leave-one-participant-out cross-validation where one participant serves as the target individual. \textbf  {(C)} Absolute improvement in F1 score (fine-tuned - base) for each participant. Dashed red line indicates no improvement ($\Delta $F1 = 0). Six of seven participants show gains (range: -0.01 to +0.06); two participants show negligible degradation within measurement noise. \textbf  {(D)} Relative improvement percentage ((fine-tuned - base) / base $\times $ 100\%) demonstrates that personalization provides proportional benefits regardless of base model performance. All results shown for models trained with 100\% target participant data and N=6 base participants.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:personalization}{{1}{5}{\textbf {Transfer learning enables robust personalization across participants.} \textbf {(A)} Population-trained base models show variable performance across participants (blue, median F1 = 0.87), while fine-tuned models achieve consistent high performance (orange, median F1 = 0.91). Boxplots show distribution across hyperparameter configurations; individual points represent specific experimental runs. \textbf {(B)} Performance improvement is consistent across all seven participants (folds 0-6). Each fold represents leave-one-participant-out cross-validation where one participant serves as the target individual. \textbf {(C)} Absolute improvement in F1 score (fine-tuned - base) for each participant. Dashed red line indicates no improvement ($\Delta $F1 = 0). Six of seven participants show gains (range: -0.01 to +0.06); two participants show negligible degradation within measurement noise. \textbf {(D)} Relative improvement percentage ((fine-tuned - base) / base $\times $ 100\%) demonstrates that personalization provides proportional benefits regardless of base model performance. All results shown for models trained with 100\% target participant data and N=6 base participants}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Data-efficient personalization requires minimal individual data.} \textbf  {(A)} Fine-tuning with target-only updates achieves high performance with as little as 5-12.5\% of target participant data. Boxplots show performance distribution across seven participants at each data percentage level (1\%, 5\%, 12.5\%, 25\%, 50\%, 100\%). \textbf  {(B)} Direct comparison of full fine-tuning (orange, continued training on base + target data) versus target-only fine-tuning (blue, training only on target data in second phase). Both approaches show similar data efficiency, with plateau effects beginning at 12.5\% target data. \textbf  {(C)} Comparison across training paradigms: full fine-tuning, target-only (trained from scratch using only target data), and target-only fine-tuning. Target-only training shows substantially lower performance at 1-5\% data, demonstrating the value of population-pretrained initialization. \textbf  {(D)} Individual participant trajectories across data percentages. Each fold maintains consistent performance trends, with diminishing returns beyond 25-50\% target data. Results demonstrate that collecting 5-12.5\% of a full individual dataset achieves near-optimal personalization.}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:data_efficiency}{{2}{6}{\textbf {Data-efficient personalization requires minimal individual data.} \textbf {(A)} Fine-tuning with target-only updates achieves high performance with as little as 5-12.5\% of target participant data. Boxplots show performance distribution across seven participants at each data percentage level (1\%, 5\%, 12.5\%, 25\%, 50\%, 100\%). \textbf {(B)} Direct comparison of full fine-tuning (orange, continued training on base + target data) versus target-only fine-tuning (blue, training only on target data in second phase). Both approaches show similar data efficiency, with plateau effects beginning at 12.5\% target data. \textbf {(C)} Comparison across training paradigms: full fine-tuning, target-only (trained from scratch using only target data), and target-only fine-tuning. Target-only training shows substantially lower performance at 1-5\% data, demonstrating the value of population-pretrained initialization. \textbf {(D)} Individual participant trajectories across data percentages. Each fold maintains consistent performance trends, with diminishing returns beyond 25-50\% target data. Results demonstrate that collecting 5-12.5\% of a full individual dataset achieves near-optimal personalization}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Moderate-sized population datasets suffice for effective transfer learning.} \textbf  {(A)} Fine-tuned model performance improves with number of base participants used for population pretraining, showing diminishing returns beyond 3-4 participants. Boxplots aggregate across seven target participants (folds), each evaluated with N=1-6 base participants. \textbf  {(B)} Base model size effect holds across different target data percentages. Performance improvements from additional base participants are most pronounced at lower target data levels (1-12.5\%), suggesting population diversity becomes less critical when abundant individual data is available. \textbf  {(C)} Individual participant responses to base model size variation. Fold 3 shows high sensitivity to base participant count, while folds 4-6 maintain robust performance even with limited base data. This heterogeneity reflects individual differences in how well participants match population-level feature representations. \textbf  {(D)} Full fine-tuning and target-only fine-tuning show similar sensitivity to base model size. Both training paradigms benefit from larger population datasets, with performance stabilizing at N=4-6 base participants. Results indicate that effective personalized models can be deployed with relatively small initial population cohorts, reducing upfront data collection costs.}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:base_ablation}{{3}{7}{\textbf {Moderate-sized population datasets suffice for effective transfer learning.} \textbf {(A)} Fine-tuned model performance improves with number of base participants used for population pretraining, showing diminishing returns beyond 3-4 participants. Boxplots aggregate across seven target participants (folds), each evaluated with N=1-6 base participants. \textbf {(B)} Base model size effect holds across different target data percentages. Performance improvements from additional base participants are most pronounced at lower target data levels (1-12.5\%), suggesting population diversity becomes less critical when abundant individual data is available. \textbf {(C)} Individual participant responses to base model size variation. Fold 3 shows high sensitivity to base participant count, while folds 4-6 maintain robust performance even with limited base data. This heterogeneity reflects individual differences in how well participants match population-level feature representations. \textbf {(D)} Full fine-tuning and target-only fine-tuning show similar sensitivity to base model size. Both training paradigms benefit from larger population datasets, with performance stabilizing at N=4-6 base participants. Results indicate that effective personalized models can be deployed with relatively small initial population cohorts, reducing upfront data collection costs}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Learned representations cluster by participant identity.} t-SNE visualization of learned feature representations from the fine-tuned model's penultimate layer. Each point represents a single 60-second window; colors indicate participant identity (folds 0-6). Clear participant-specific clustering demonstrates that fine-tuning adapts internal representations to capture individual behavioral patterns. Distinct clusters for each participant (particularly visible for participants 2, 3, and 6) suggest the model learns participant-specific smoking gesture signatures, explaining improved personalized performance. Overlapping regions in the center indicate shared features across participants (e.g., universal hand-to-mouth motion patterns), while peripheral clusters capture individual-specific nuances (e.g., hand dominance, device placement, smoking style). This representation structure illustrates how transfer learning balances population-level generalization with individual-specific adaptation.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:tsne}{{4}{8}{\textbf {Learned representations cluster by participant identity.} t-SNE visualization of learned feature representations from the fine-tuned model's penultimate layer. Each point represents a single 60-second window; colors indicate participant identity (folds 0-6). Clear participant-specific clustering demonstrates that fine-tuning adapts internal representations to capture individual behavioral patterns. Distinct clusters for each participant (particularly visible for participants 2, 3, and 6) suggest the model learns participant-specific smoking gesture signatures, explaining improved personalized performance. Overlapping regions in the center indicate shared features across participants (e.g., universal hand-to-mouth motion patterns), while peripheral clusters capture individual-specific nuances (e.g., hand dominance, device placement, smoking style). This representation structure illustrates how transfer learning balances population-level generalization with individual-specific adaptation}{figure.4}{}}
\citation{yosinski2014transfer,devlin2019bert}
\citation{saleheen2015puffmarker}
\citation{parate2014detecting}
\citation{mathur2021designing}
\citation{settles2009active}
\citation{finn2017model}
\@writefile{toc}{\contentsline {section}{\numberline {3}Discussion}{9}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{10}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Participants and Data Collection}{10}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Data Preprocessing and Windowing}{10}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Model Architecture}{10}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Training Procedures}{11}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Experimental Design and Hyperparameters}{11}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Evaluation Metrics}{11}{subsection.4.6}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{references}
\bibcite{who2021tobacco}{{1}{}{{}}{{}}}
\bibcite{nahum2018just}{{2}{}{{}}{{}}}
\bibcite{sazonov2013smoke}{{3}{}{{}}{{}}}
\bibcite{saleheen2015puffmarker}{{4}{}{{}}{{}}}
\bibcite{tang2020individualized}{{5}{}{{}}{{}}}
\bibcite{stone2007patient}{{6}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Data Availability}{12}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Code Availability}{12}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgements}{12}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Author Contributions}{12}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Competing Interests}{12}{section.9}\protected@file@percent }
\bibcite{yosinski2014transfer}{{7}{}{{}}{{}}}
\bibcite{devlin2019bert}{{8}{}{{}}{{}}}
\bibcite{parate2014detecting}{{9}{}{{}}{{}}}
\bibcite{mathur2021designing}{{10}{}{{}}{{}}}
\bibcite{settles2009active}{{11}{}{{}}{{}}}
\bibcite{finn2017model}{{12}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{13}
