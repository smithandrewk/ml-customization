{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50413aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lib.utils import (\n",
    "    load_config,\n",
    "    get_experiment_dir,\n",
    "    make_windowed_dataset_from_sessions,\n",
    "    get_participant_id,\n",
    "    get_participant_projects,\n",
    "    get_raw_dataset_path,\n",
    "    get_sessions_for_project,\n",
    "    generate_dataset_summary\n",
    ")\n",
    "\n",
    "participant = 'ejaz'\n",
    "\n",
    "participant_X_train = []\n",
    "participant_y_train = []\n",
    "participant_X_test = []\n",
    "participant_y_test = []\n",
    "    \n",
    "# Get participant_id from participant code\n",
    "participant_id = get_participant_id(participant)\n",
    "print(f\"Participant ID: {participant_id}\")\n",
    "    \n",
    "# Get projects for the participant\n",
    "projects = get_participant_projects(participant_id)\n",
    "print(f\"Projects for {participant}: {projects}\")\n",
    "\n",
    "sessions = []\n",
    "for project_name in projects:\n",
    "    print(f\"Processing project: {project_name}\")\n",
    "    raw_dataset_path = get_raw_dataset_path(project_name)\n",
    "    ss = [s for s in get_sessions_for_project(project_name) if s.get('keep') != 0 and s.get('smoking_verified') == 1]\n",
    "    for s in ss:\n",
    "        s['raw_dataset_path'] = raw_dataset_path\n",
    "    sessions.extend(ss)\n",
    "\n",
    "# 3-way split for target participants: 60% train, 20% val, 20% test\n",
    "train_sessions, temp_sessions = train_test_split(\n",
    "    sessions,\n",
    "    test_size=0.2,  # 40% for val+test\n",
    "    random_state=42\n",
    ")\n",
    "val_sessions, test_sessions = train_test_split(\n",
    "    temp_sessions,\n",
    "    test_size=0.5,  # 50% of 40% = 20% total for test\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_sessions = val_sessions + test_sessions\n",
    "print(f\"Target participant split - Train: {len(train_sessions)}, Val: {len(val_sessions)}\")\n",
    "\n",
    "X_train, y_train = make_windowed_dataset_from_sessions(\n",
    "    sessions=train_sessions,\n",
    "    window_size=3000,\n",
    "    window_stride=3000,\n",
    "    raw_dataset_path=raw_dataset_path,\n",
    "    labeling='andrew smoking labels',\n",
    "    sensor_config={'use_accelerometer': True, 'use_gyroscope': True}\n",
    ")\n",
    "\n",
    "X_val, y_val = make_windowed_dataset_from_sessions(\n",
    "    sessions=val_sessions,\n",
    "    window_size=3000,\n",
    "    window_stride=3000,\n",
    "    raw_dataset_path=raw_dataset_path,\n",
    "    labeling='andrew smoking labels',\n",
    "    sensor_config={'use_accelerometer': True, 'use_gyroscope': True}\n",
    ")\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_val = y_val.reshape(-1,1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "class ObnoxiouslySimpleCNN(nn.Module):\n",
    "    def __init__(self, input_channels, base_channels, dropout):\n",
    "        super(ObnoxiouslySimpleCNN, self).__init__()\n",
    "        self.stem = nn.Conv1d(input_channels, base_channels, kernel_size=7, padding=1) # 3000 -> 2994, receptive field 7\n",
    "        self.conv1 = nn.Conv1d(base_channels, base_channels, kernel_size=5, padding=2) # 2994 -> 2994, receptive field 11\n",
    "        self.conv2 = nn.Conv1d(base_channels, base_channels, kernel_size=5, padding=2) # 2994 -> 2994, receptive field 15\n",
    "        self.conv3 = nn.Conv1d(base_channels, base_channels, kernel_size=5, padding=2) # 2994 -> 2994, receptive field 19\n",
    "        self.conv4 = nn.Conv1d(base_channels, base_channels, kernel_size=5, padding=2) # 2994 -> 2994, receptive field 23\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(base_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = relu(x)\n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)        \n",
    "        return x\n",
    "\n",
    "model = ObnoxiouslySimpleCNN(input_channels=6, base_channels=8, dropout=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "criterion = criterion.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9426990",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossi = []\n",
    "val_lossi = []\n",
    "val_f1i = []\n",
    "val_i = []\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78453939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.train()\n",
    "\n",
    "for _ in range(500):\n",
    "    loss_epoch = 0\n",
    "    for Xi,yi in trainloader:\n",
    "        Xi = Xi.to('cuda')\n",
    "        yi = yi.to('cuda')\n",
    "        logits = model(Xi)\n",
    "        loss = criterion(logits, yi)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch += loss.item()\n",
    "    loss_epoch /= len(trainloader)\n",
    "    lossi.append(loss_epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        val_loss_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xi, yi in valloader:\n",
    "                Xi = Xi.to('cuda')\n",
    "                yi = yi.to('cuda')\n",
    "                logits = model(Xi)\n",
    "                loss = criterion(logits, yi)\n",
    "                val_loss_total += loss.item()\n",
    "\n",
    "                all_preds.append(torch.sigmoid(logits).round().cpu())\n",
    "                all_labels.append(yi.cpu())\n",
    "\n",
    "        val_loss = val_loss_total / len(valloader)\n",
    "        val_f1 = f1_score(\n",
    "        torch.cat(all_labels).numpy(),\n",
    "        torch.cat(all_preds).numpy(),\n",
    "        average='macro'\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        val_lossi.append(val_loss)\n",
    "        val_i.append(epoch)\n",
    "        val_f1i.append(val_f1)\n",
    "\n",
    "        fig,ax= plt.subplots(nrows=2,ncols=1,figsize=(7.2,10))\n",
    "        ax[0].plot(lossi)\n",
    "        ax[0].plot(val_i, val_lossi, color='red')\n",
    "        ax[0].set_yscale('log')\n",
    "\n",
    "        ax[1].plot(val_i, val_f1i, color='green')\n",
    "\n",
    "        plt.savefig(f'loss.png')\n",
    "        plt.close()\n",
    "        \n",
    "    print(f'Epoch {epoch}: train loss {loss_epoch:.4f}, val loss {val_loss:.4f}, val f1 {val_f1:.4f}, lr {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    model.train()\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e89eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        val_loss_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for Xi, yi in valloader:\n",
    "                Xi = Xi.to('cuda')\n",
    "                yi = yi.to('cuda')\n",
    "                logits = model(Xi)\n",
    "                loss = criterion(logits, yi)\n",
    "                val_loss_total += loss.item()\n",
    "\n",
    "                all_preds.append(torch.sigmoid(logits).round().cpu())\n",
    "                all_labels.append(yi.cpu())\n",
    "\n",
    "        val_loss = val_loss_total / len(valloader)\n",
    "        val_f1 = f1_score(\n",
    "        torch.cat(all_labels).numpy(),\n",
    "        torch.cat(all_preds).numpy(),\n",
    "        average='macro'\n",
    "        )\n",
    "\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        val_lossi.append(val_loss)\n",
    "        val_i.append(epoch)\n",
    "        val_f1i.append(val_f1)\n",
    "\n",
    "        fig,ax= plt.subplots(nrows=2,ncols=1,figsize=(7.2,10))\n",
    "        ax[0].plot(lossi)\n",
    "        ax[0].plot(val_i, val_lossi, color='red')\n",
    "        ax[0].set_yscale('log')\n",
    "\n",
    "        ax[1].plot(val_i, val_f1i, color='green')\n",
    "\n",
    "        plt.savefig(f'loss.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48137c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one of x_train sessions in time domain with model predictions\n",
    "session = val_sessions[0]\n",
    "labeling = f'andrew smoking labels'\n",
    "window_size = 3000\n",
    "window_stride = 3000\n",
    "\n",
    "session_name = session['session_name']\n",
    "raw_dataset_path = session['raw_dataset_path']\n",
    "start_ns = session.get('start_ns')\n",
    "stop_ns = session.get('stop_ns')\n",
    "\n",
    "print(f\"Generating windowed dataset for session: {session_name}\")\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "sensor_config = {'use_accelerometer': True, 'use_gyroscope': True}\n",
    "\n",
    "# Determine which columns to use based on sensor config\n",
    "sensor_columns = []\n",
    "if sensor_config.get('use_accelerometer', True):\n",
    "    sensor_columns.extend(['accel_x', 'accel_y', 'accel_z'])\n",
    "if sensor_config.get('use_gyroscope', False):\n",
    "    sensor_columns.extend(['gyro_x', 'gyro_y', 'gyro_z'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bouts = [b for b in session['bouts'] if b['label'] == labeling]\n",
    "\n",
    "from lib.utils import load_data, resample\n",
    "\n",
    "df = load_data(raw_dataset_path, session_name, sensor_config, start_ns, stop_ns)\n",
    "df = resample(df)\n",
    "df['label'] = 0\n",
    "\n",
    "for bout in bouts:\n",
    "    start = bout['start']\n",
    "    end = bout['end']\n",
    "    df.loc[(df['ns_since_reboot'] >= start) & (df['ns_since_reboot'] <= end), 'label'] = 1\n",
    "\n",
    "if 'accel_x' not in df.columns and sensor_config.get('use_accelerometer', True):\n",
    "    if 'x' in df.columns:\n",
    "        df.rename(columns={'x': 'accel_x', 'y': 'accel_y', 'z': 'accel_z'}, inplace=True)\n",
    "data_columns = sensor_columns + ['label']\n",
    "missing_columns = [col for col in data_columns if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Warning: Missing columns {missing_columns} in session {session_name}\")\n",
    "    raise ValueError(f\"Missing columns {missing_columns} in session {session_name}\")\n",
    "\n",
    "data = torch.tensor(df[data_columns].values, dtype=torch.float32)\n",
    "\n",
    "if data.shape[0] < window_size:\n",
    "    # Zero pad the data to window size\n",
    "    padding_length = window_size - data.shape[0]\n",
    "    padding = torch.zeros((padding_length, data.shape[1]), dtype=torch.float32)\n",
    "    data = torch.cat([data, padding], dim=0)\n",
    "    print(f\"Zero-padded session {session_name} from {data.shape[0] - padding_length} to {data.shape[0]} samples\")\n",
    "\n",
    "windowed_data = data.unfold(dimension=0,size=window_size,step=250)\n",
    "X.append(windowed_data[:,:-1,:])\n",
    "y.append(windowed_data[:,-1,:])\n",
    "\n",
    "X = torch.cat(X)\n",
    "y = (~(torch.cat(y) == 0).all(axis=1)).float()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X.to('cuda'))\n",
    "    predictions = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "\n",
    "window_size = 3000\n",
    "step = 250\n",
    "\n",
    "num_windows = predictions.shape[0]  # 907\n",
    "\n",
    "# Initialize output\n",
    "time_domain_length = (num_windows - 1) * step + window_size  # 909000\n",
    "time_domain_preds = torch.zeros(time_domain_length)\n",
    "overlap_counts = torch.zeros(time_domain_length)\n",
    "\n",
    "# Accumulate predictions\n",
    "for i, pred in enumerate(predictions):\n",
    "    start_idx = i * step\n",
    "    end_idx = start_idx + window_size\n",
    "    time_domain_preds[start_idx:end_idx] += pred.item()\n",
    "    overlap_counts[start_idx:end_idx] += 1\n",
    "\n",
    "# Average where overlapping\n",
    "time_domain_preds = time_domain_preds / overlap_counts\n",
    "\n",
    "df = df.iloc[:len(time_domain_preds)]\n",
    "df['logits'] = time_domain_preds\n",
    "df['y_pred'] = (time_domain_preds > .9).int()\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "y_true = df['label'].values\n",
    "y_pred = df['y_pred'].astype(int).values\n",
    "ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "fig = px.line(df.iloc[::20], x='ns_since_reboot', y=['accel_x','accel_y','accel_z','label','y_pred','logits'], title=f'Session {session_name} Smoking Prediction')\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee79d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
