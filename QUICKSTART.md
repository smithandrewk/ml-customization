# Quick Reference: After Running Experiments

## ğŸš€ One-Line Solution

After your training completes, just run:

```bash
./post_experiment_evaluation.sh --all
```

**This automatically:**
1. âœ“ Computes test set F1 scores for all experiments
2. âœ“ Generates manuscript results (results.tex, abstract_stats.tex)
3. âœ“ Creates all figures with test set data
4. âœ“ Produces a summary report

## ğŸ“‹ What This Fixes

### âŒ Before (Using Validation Metrics - WRONG):
```python
# figure2.py, figure3.py, figure4.py
base_f1 = metrics['best_target_val_f1_from_best_base_model']  # BIASED
target_f1 = metrics['best_target_val_f1']                      # BIASED
```

### âœ… After (Using Test Metrics - CORRECT):
```python
# All figure scripts now use:
base_f1 = metrics['base_test_f1']    # UNBIASED - held-out test set
target_f1 = metrics['test_f1']       # UNBIASED - held-out test set
```

## ğŸ“Š Expected Output

The script will:

1. **Evaluate test sets** (2-3 minutes)
   ```
   âœ“ Base model test F1: 0.8672
   âœ“ Personalized model test F1: 0.8622
   âœ“ Improvement: -0.0050 (-0.6%)
   ```

2. **Generate manuscript files**
   - `manuscript/results.tex` - Tables with test set performance
   - `manuscript/abstract_stats.tex` - Stats for abstract

3. **Create figures**
   - `figures/figure2.pdf` - Main results
   - `figures/figure3.pdf` - Training dynamics
   - `figures/figure4.pdf` - Data efficiency

4. **Save summary report**
   - `evaluation_report_YYYYMMDD_HHMMSS.txt`

## ğŸ”§ Advanced Usage

### Evaluate specific experiment only:
```bash
./post_experiment_evaluation.sh my_experiment_name
```

### Use different GPU:
```bash
./post_experiment_evaluation.sh --all --device cuda:1
```

### Manual evaluation (if script fails):
```bash
# Step 1: Compute test metrics
python3 evaluate_on_test_set.py --all --device cuda:0

# Step 2: Generate results
cd manuscript
python3 generate_results.py my_experiment_name

# Step 3: Generate figures
python3 figure2.py --experiment my_experiment_name
python3 figure3.py --experiment my_experiment_name
python3 figure4.py
cd ..

# Step 4: Copy figures
cd figures
cp figure2_my_experiment_name.pdf figure2.pdf
cp figure3_my_experiment_name.pdf figure3.pdf
```

## ğŸ“ What Gets Updated

```
experiments/
â””â”€â”€ my_experiment/
    â””â”€â”€ fold0_participant/
        â””â”€â”€ metrics.json        # â† NEW: base_test_f1, test_f1 added

manuscript/
â”œâ”€â”€ results.tex                 # â† UPDATED: test set metrics
â”œâ”€â”€ abstract_stats.tex          # â† UPDATED: test set metrics
â””â”€â”€ manuscript.pdf              # â† Recompile after running script

figures/
â”œâ”€â”€ figure2.pdf                 # â† UPDATED: test set metrics
â”œâ”€â”€ figure3.pdf                 # â† UPDATED: test set metrics
â””â”€â”€ figure4.pdf                 # â† UPDATED: test set metrics
```

## âš ï¸ Important Notes

### Always Run This Script When:
- âœ“ New experiments complete
- âœ“ Re-running experiments
- âœ“ Before compiling manuscript
- âœ“ Before generating any figures

### Never Use:
- âŒ Validation metrics in manuscript
- âŒ Figures generated before running this script
- âŒ Results tables without test set evaluation

## ğŸ¯ For Full n=15 Experiments

**Before training:**
1. Edit `train.py` line 38 to include all 15 participants
2. Run training for all 15 folds

**After training:**
```bash
./post_experiment_evaluation.sh --all
```

Done! All results will use test set metrics.

## ğŸ’¡ Why This Matters

**Validation Set Results (n=3):**
- Responder rate: 100% (3/3) â† Optimistic!
- Mean improvement: +9.8%    â† Inflated!

**Test Set Results (n=3):**
- Responder rate: 33% (1/3)  â† Realistic
- Mean improvement: +5.7%    â† Honest

**Difference:** Validation metrics showed overfitting for 2/3 participants!

---

**See `WORKFLOW.md` for detailed documentation.**