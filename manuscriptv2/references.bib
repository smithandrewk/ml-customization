@article{akbariPersonalizingActivityRecognition2020,
  title = {Personalizing {{Activity Recognition Models Through Quantifying Different Types}} of {{Uncertainty Using Wearable Sensors}}},
  author = {Akbari, Ali and Jafari, Roozbeh},
  year = 2020,
  month = sep,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {67},
  number = {9},
  pages = {2530--2541},
  issn = {0018-9294, 1558-2531},
  doi = {10.1109/TBME.2019.2963816},
  urldate = {2025-11-03},
  abstract = {Recognizing activities of daily living (ADL) provides vital contextual information that enhances the effectiveness of various mobile health and wellness applications. Development of wearable motion sensors along with machine learning algorithms offer a great opportunity for ADL recognition. However, the performance of the ADL recognition systems may significantly degrade when they are used by a new user due to inter-subject variability. This issue limits the usability of these systems. In this paper, we propose a deep learning assisted personalization framework for ADL recognition with the aim to maximize the personalization performance while minimizing solicitation of inputs or labels from the user to reduce user's burden. The proposed framework consists of unsupervised retraining of automatic feature extraction layers and supervised fine-tuning of classification layers through a novel active learning model based on a given model's uncertainty. We design a Bayesian deep convolutional neural network with stochastic latent variables that allows us to estimate both aleatoric (data-dependent) and epistemic (model-dependent) uncertainties in recognition task. In this study, for the first time, we show how distinguishing between the two aforementioned sources of uncertainty leads to more effective active learning. The experimental results show that our proposed method improves the accuracy of ADL recognition on a new user by 25\% on average compared to the case of using a model for a new user with no personalization with an average final accuracy of 89.2\%. Moreover, our method achieves higher personalization accuracy while significantly reducing user's burden in terms of soliciting inputs and labels compared to other methods.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/andrew/Zotero/storage/LVTS4ZTV/Akbari and Jafari - 2020 - Personalizing Activity Recognition Models Through Quantifying Different Types of Uncertainty Using W.pdf}
}

@article{coleDetectingSmokingEvents2017,
  title = {Detecting {{Smoking Events Using Accelerometer Data Collected Via Smartwatch Technology}}: {{Validation Study}}},
  shorttitle = {Detecting {{Smoking Events Using Accelerometer Data Collected Via Smartwatch Technology}}},
  author = {Cole, Casey A and Anshari, Dien and Lambert, Victoria and Thrasher, James F and Valafar, Homayoun},
  year = 2017,
  month = dec,
  journal = {JMIR mHealth and uHealth},
  volume = {5},
  number = {12},
  pages = {e189},
  issn = {2291-5222},
  doi = {10.2196/mhealth.9035},
  urldate = {2025-11-03},
  abstract = {Background Smoking is the leading cause of preventable death in the world today. Ecological research on smoking in context currently relies on self-reported smoking behavior. Emerging smartwatch technology may more objectively measure smoking behavior by automatically detecting smoking sessions using robust machine learning models. Objective This study aimed to examine the feasibility of detecting smoking behavior using smartwatches. The second aim of this study was to compare the success of observing smoking behavior with smartwatches to that of conventional self-reporting. Methods A convenience sample of smokers was recruited for this study. Participants (N=10) recorded 12 hours of accelerometer data using a mobile phone and smartwatch. During these 12 hours, they engaged in various daily activities, including smoking, for which they logged the beginning and end of each smoking session. Raw data were classified as either smoking or nonsmoking using a machine learning model for pattern recognition. The accuracy of the model was evaluated by comparing the output with a detailed description of a modeled smoking session. Results In total, 120 hours of data were collected from participants and analyzed. The accuracy of self-reported smoking was approximately 78\% (96/123). Our model was successful in detecting 100 of 123 (81\%) smoking sessions recorded by participants. After eliminating sessions from the participants that did not adhere to study protocols, the true positive detection rate of the smartwatch based-detection increased to more than 90\%. During the 120 hours of combined observation time, only 22 false positive smoking sessions were detected resulting in a 2.8\% false positive rate. Conclusions Smartwatch technology can provide an accurate, nonintrusive means of monitoring smoking behavior in natural contexts. The use of machine learning algorithms for passively detecting smoking sessions may enrich ecological momentary assessment protocols and cessation intervention studies that often rely on self-reported behaviors and may not allow for targeted data collection and communications around smoking events.},
  pmcid = {PMC5745355},
  pmid = {29237580}
}

@misc{dhekaneTransferLearningHuman2024,
  title = {Transfer {{Learning}} in {{Human Activity Recognition}}: {{A Survey}}},
  shorttitle = {Transfer {{Learning}} in {{Human Activity Recognition}}},
  author = {Dhekane, Sourish Gunesh and Ploetz, Thomas},
  year = 2024,
  month = jan,
  number = {arXiv:2401.10185},
  eprint = {2401.10185},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.10185},
  urldate = {2025-11-03},
  abstract = {Sensor-based human activity recognition (HAR) has been an active research area, owing to its applications in smart environments, assisted living, fitness, healthcare, etc. Recently, deep learning based end-to-end training has resulted in state-of-the-art performance in domains such as computer vision and natural language, where large amounts of annotated data are available. However, large quantities of annotated data are not available for sensor-based HAR. Moreover, the real-world settings on which the HAR is performed differ in terms of sensor modalities, classification tasks, and target users. To address this problem, transfer learning has been employed extensively. In this survey, we focus on these transfer learning methods in the application domains of smart home and wearables-based HAR. In particular, we provide a problem-solution perspective by categorizing and presenting the works in terms of their contributions and the challenges they address. We also present an updated view of the state-of-the-art for both application domains. Based on our analysis of 205 papers, we highlight the gaps in the literature and provide a roadmap for addressing them. This survey provides a reference to the HAR community, by summarizing the existing works and providing a promising research agenda.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/andrew/Zotero/storage/CWIEZ8Q7/Dhekane and Ploetz - 2024 - Transfer Learning in Human Activity Recognition A Survey.pdf}
}

@misc{PersonalizedHumanActivity,
  title = {Personalized {{Human Activity Recognition Based}} on {{Integrated Wearable Sensor}} and {{Transfer Learning}}},
  urldate = {2025-11-03},
  howpublished = {https://www.mdpi.com/1424-8220/21/3/885},
  file = {/home/andrew/Zotero/storage/RGDTMRIX/885.html}
}

@article{senyurekCNNLSTMNeuralNetwork2020,
  title = {A {{CNN-LSTM}} Neural Network for Recognition of Puffing in Smoking Episodes Using Wearable Sensors},
  author = {Senyurek, Volkan Y. and Imtiaz, Masudul H. and Belsare, Prajakta and Tiffany, Stephen and Sazonov, Edward},
  year = 2020,
  month = jan,
  journal = {Biomedical Engineering Letters},
  volume = {10},
  number = {2},
  pages = {195--203},
  issn = {2093-9868},
  doi = {10.1007/s13534-020-00147-8},
  urldate = {2025-11-03},
  abstract = {A detailed assessment of smoking behavior under free-living conditions is a key challenge for health behavior research. A number of methods using wearable sensors and puff topography devices have been developed for smoking and individual puff detection. In this paper, we propose a novel algorithm for automatic detection of puffs in smoking episodes by using a combination of Respiratory Inductance Plethysmography and Inertial Measurement Unit sensors. The detection of puffs was performed by using a deep network containing convolutional and recurrent neural networks. Convolutional neural networks (CNN) were utilized to automate feature learning from raw sensor streams. Long Short Term Memory (LSTM) network layers were utilized to obtain the temporal dynamics of sensor signals and classify sequence of time segmented sensor streams. An evaluation was performed by using a large, challenging dataset containing 467 smoking events from 40 participants under free-living conditions. The proposed approach achieved an F1-score of 78\% in leave-one-subject-out cross-validation. The results suggest that CNN-LSTM based neural network architecture sufficiently detect puffing episodes in free-living condition. The proposed model be used as a detection tool for smoking cessation programs and scientific research.},
  pmcid = {PMC7235127},
  pmid = {32431952},
  file = {/home/andrew/Zotero/storage/AZ2IPDJ7/Senyurek et al. - 2020 - A CNN-LSTM neural network for recognition of puffing in smoking episodes using wearable sensors.pdf}
}

@misc{thukralCrossDomainHARFew2023,
  title = {Cross-{{Domain HAR}}: {{Few Shot Transfer Learning}} for {{Human Activity Recognition}}},
  shorttitle = {Cross-{{Domain HAR}}},
  author = {Thukral, Megha and Haresamudram, Harish and Ploetz, Thomas},
  year = 2023,
  month = oct,
  number = {arXiv:2310.14390},
  eprint = {2310.14390},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.14390},
  urldate = {2025-11-03},
  abstract = {The ubiquitous availability of smartphones and smartwatches with integrated inertial measurement units (IMUs) enables straightforward capturing of human activities. For specific applications of sensor based human activity recognition (HAR), however, logistical challenges and burgeoning costs render especially the ground truth annotation of such data a difficult endeavor, resulting in limited scale and diversity of datasets. Transfer learning, i.e., leveraging publicly available labeled datasets to first learn useful representations that can then be fine-tuned using limited amounts of labeled data from a target domain, can alleviate some of the performance issues of contemporary HAR systems. Yet they can fail when the differences between source and target conditions are too large and/ or only few samples from a target application domain are available, each of which are typical challenges in real-world human activity recognition scenarios. In this paper, we present an approach for economic use of publicly available labeled HAR datasets for effective transfer learning. We introduce a novel transfer learning framework, Cross-Domain HAR, which follows the teacher-student self-training paradigm to more effectively recognize activities with very limited label information. It bridges conceptual gaps between source and target domains, including sensor locations and type of activities. Through our extensive experimental evaluation on a range of benchmark datasets, we demonstrate the effectiveness of our approach for practically relevant few shot activity recognition scenarios. We also present a detailed analysis into how the individual components of our framework affect downstream performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/andrew/Zotero/storage/XIL3LNQY/Thukral et al. - 2023 - Cross-Domain HAR Few Shot Transfer Learning for Human Activity Recognition.pdf;/home/andrew/Zotero/storage/FRMMDZJ7/2310.html}
}

@article{yangDeepLearningTransfer2022,
  title = {Deep Learning and Transfer Learning for Device-Free Human Activity Recognition: {{A}} Survey},
  shorttitle = {Deep Learning and Transfer Learning for Device-Free Human Activity Recognition},
  author = {Yang, Jianfei and Xu, Yuecong and Cao, Haozhi and Zou, Han and Xie, Lihua},
  year = 2022,
  month = dec,
  journal = {Journal of Automation and Intelligence},
  volume = {1},
  number = {1},
  pages = {100007},
  issn = {2949-8554},
  doi = {10.1016/j.jai.2022.100007},
  urldate = {2025-11-03},
  abstract = {Device-free activity recognition plays a crucial role in smart building, security, and human--computer interaction, which shows its strength in its convenience and cost-efficiency. Traditional machine learning has made significant progress by heuristic hand-crafted features and statistical models, but it suffers from the limitation of manual feature design. Deep learning overcomes such issues by automatic high-level feature extraction, but its performance degrades due to the requirement of massive annotated data and cross-site issues. To deal with these problems, transfer learning helps to transfer knowledge from existing datasets while dealing with the negative effect of background dynamics. This paper surveys the recent progress of deep learning and transfer learning for device-free activity recognition. We begin with the motivation of deep learning and transfer learning, and then introduce the major sensor modalities. Then the deep and transfer learning techniques for device-free human activity recognition are introduced. Eventually, insights on existing works and grand challenges are summarized and presented to promote future research.},
  keywords = {Action recognition,Deep learning,Device-free,Domain adaptation,Human activity recognition,Transfer learning},
  file = {/home/andrew/Zotero/storage/ZKEY4HJ8/Yang et al. - 2022 - Deep learning and transfer learning for device-free human activity recognition A survey.pdf;/home/andrew/Zotero/storage/SEYVWUT5/S2949855422000077.html}
}

@article{zhangDeepLearningHuman2022,
  title = {Deep {{Learning}} in {{Human Activity Recognition}} with {{Wearable Sensors}}: {{A Review}} on {{Advances}}},
  shorttitle = {Deep {{Learning}} in {{Human Activity Recognition}} with {{Wearable Sensors}}},
  author = {Zhang, Shibo and Li, Yaxuan and Zhang, Shen and Shahabi, Farzad and Xia, Stephen and Deng, Yu and Alshurafa, Nabil},
  year = 2022,
  month = jan,
  journal = {Sensors},
  volume = {22},
  number = {4},
  pages = {1476},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22041476},
  urldate = {2025-11-03},
  abstract = {Mobile and wearable devices have enabled numerous applications, including activity tracking, wellness monitoring, and human--computer interaction, that measure and improve our daily lives. Many of these applications are made possible by leveraging the rich collection of low-power sensors found in many mobile and wearable devices to perform human activity recognition (HAR). Recently, deep learning has greatly pushed the boundaries of HAR on mobile and wearable devices. This paper systematically categorizes and summarizes existing work that introduces deep learning methods for wearables-based HAR and provides a comprehensive analysis of the current advancements, developing trends, and major challenges. We also present cutting-edge frontiers and future directions for deep learning-based HAR.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,human activity recognition,pervasive computing,review,ubiquitous computing,wearable sensors},
  file = {/home/andrew/Zotero/storage/4MU7N3XQ/Zhang et al. - 2022 - Deep Learning in Human Activity Recognition with Wearable Sensors A Review on Advances.pdf}
}
