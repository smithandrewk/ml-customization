# Advanced Customization Techniques - Ablation Study Configuration
# This config is optimized for systematic ablation studies

# Dataset Configuration
dataset:
  fs: 50  # Sampling frequency (Hz)
  window_size_seconds: 60  # Window size in seconds
  window_stride_seconds: 30  # Window stride in seconds
  labeling: "andrew smoking labels"  # Label type to use
  test_size: 0.2  # Train/test split ratio

# Sensor Configuration
sensors:
  use_accelerometer: true   # Include accelerometer data (accel_x, accel_y, accel_z)
  use_gyroscope: true       # Include gyroscope data (gyro_x, gyro_y, gyro_z)

# Model Configuration (computed automatically)
model: {}

# Training Hyperparameters
training:
  batch_size: 512
  learning_rate: 0.0003  # 3e-4
  weight_decay: 0.0001   # 1e-4
  num_epochs: 200  # Reduced for faster ablation studies
  
  # Early stopping parameters (aggressive for ablation studies)
  base_patience: 20      # Early stopping patience for base training phase
  custom_patience: 15    # Early stopping patience for customization phase
  custom_learning_rate: 0.00003  # Lower LR for fine-tuning (10x lower than base)
  target_weight_multiplier: 5.0  # Weight multiplier for target samples in Phase 2
  
  # Advanced customization techniques (for ablation studies)
  advanced_techniques:
    # Layer-wise fine-tuning
    use_layerwise_finetuning: false
    layerwise_classifier_epochs: 3  # Reduced for faster experiments
    layerwise_lr_multiplier: 0.1    # LR multiplier for frozen layers when unfrozen
    
    # Gradual unfreezing
    use_gradual_unfreezing: false
    unfreeze_schedule: [3, 6, 9]     # Earlier unfreezing for shorter experiments
    
    # Elastic Weight Consolidation (EWC)
    use_ewc: false
    ewc_lambda: 1000.0               # EWC regularization strength
    fisher_samples: 500              # Reduced for faster computation
    
    # Data augmentation
    use_augmentation: false
    augmentation:
      jitter_noise_std: 0.01         # Standard deviation for jittering noise
      magnitude_scale_range: [0.9, 1.1]  # Range for magnitude scaling
      time_warp_sigma: 0.2           # Sigma for time warping
      augmentation_probability: 0.5   # Probability of applying augmentation
      
    # CORAL domain adaptation
    use_coral: false
    coral_lambda: 1.0                # CORAL loss weight
    
    # Ensemble approach
    use_ensemble: false
    ensemble_alpha: 0.7              # Weight for base model in ensemble (0.7 base + 0.3 target)
    
    # Contrastive learning
    use_contrastive: false
    contrastive_lambda: 0.1          # Contrastive loss weight
    contrastive_temperature: 0.1     # Temperature for contrastive loss

# Participants Configuration
participants:
  - "tonmoy"
  - "asfik"
  - "ejaz"
  - "alsaad"
  - "anam"
  - "iftakhar"
  - "unk1"

# Visualization Configuration
visualization:
  ma_window_size: 3  # Smaller window for shorter experiments
  save_plots: true
  plot_format: "png"  # PNG for faster generation during ablation
  dpi: 150           # Lower DPI for faster generation

# ============================================================================
# TECHNIQUE-SPECIFIC CONFIGURATIONS
# ============================================================================

# High-performance configuration (for best results)
high_performance:
  training:
    num_epochs: 500
    base_patience: 40
    custom_patience: 30
    advanced_techniques:
      ewc_lambda: 2000.0
      fisher_samples: 1000
      layerwise_classifier_epochs: 5
      coral_lambda: 2.0
      contrastive_lambda: 0.2

# Fast configuration (for rapid prototyping)
fast:
  training:
    num_epochs: 50
    base_patience: 5
    custom_patience: 5
    advanced_techniques:
      fisher_samples: 200
      layerwise_classifier_epochs: 2
      unfreeze_schedule: [2, 4]

# Conservative configuration (for stability)
conservative:
  training:
    learning_rate: 0.0001
    custom_learning_rate: 0.00001
    advanced_techniques:
      ewc_lambda: 500.0
      coral_lambda: 0.5
      contrastive_lambda: 0.05
      layerwise_lr_multiplier: 0.05

# Memory-efficient configuration (for limited resources)
memory_efficient:
  training:
    batch_size: 256
    advanced_techniques:
      fisher_samples: 200
      augmentation:
        augmentation_probability: 0.3