{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using Testing Data Instead Of Validation\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from lib.utils_simple import evaluate, compute_loss_and_f1\n",
    "from lib.models import ConvLayerNorm, Block\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.blocks = []\n",
    "        self.blocks.append(Block(6,8))\n",
    "        for _ in range(5):\n",
    "            self.blocks.append(Block(8,8))\n",
    "            self.blocks.append(Block(8,8,pool=False))\n",
    "\n",
    "        self.blocks.append(Block(8,16,pool=False))\n",
    "\n",
    "        # for _ in range(5):\n",
    "        #     self.blocks.append(Block(16,16))\n",
    "        #     self.blocks.append(Block(16,16,pool=False))\n",
    "            \n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Load all experiments into a dataframe\n",
    "performances = {}\n",
    "from tqdm import tqdm\n",
    "\n",
    "experiments_dir = f'./experiments_all_participants'\n",
    "\n",
    "for experiment in tqdm(os.listdir(experiments_dir)):\n",
    "    base_f1s = []\n",
    "    target_f1s = []\n",
    "    folds = []\n",
    "\n",
    "    for run in os.listdir(f'{experiments_dir}/{experiment}'):\n",
    "        if not os.path.exists(f'{experiments_dir}/{experiment}/{run}/metrics.json'):\n",
    "            continue\n",
    "        metrics = json.load(open(f'{experiments_dir}/{experiment}/{run}/metrics.json'))\n",
    "        losses = json.load(open(f'{experiments_dir}/{experiment}/{run}/losses.json'))\n",
    "        hyperparameters = json.load(open(f'{experiments_dir}/{experiment}/{run}/hyperparameters.json'))\n",
    "\n",
    "        data_path = hyperparameters['data_path']\n",
    "        target_participant = hyperparameters['target_participant']\n",
    "        target_testloader = DataLoader(TensorDataset(*torch.load(f'data/001_60s_window/{target_participant}_test.pt')), batch_size=128, shuffle=False)\n",
    "\n",
    "        model = TestModel()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if 'target_only' in experiment:\n",
    "            base_model_on_target_val_f1 = None\n",
    "            base_f1s.append(base_model_on_target_val_f1)\n",
    "            model.load_state_dict(torch.load(f'{experiments_dir}/{experiment}/{run}/best_base_model.pt', map_location='cpu')) # naming is an artifact of training mode\n",
    "            _,target_model_on_target_val_f1 = compute_loss_and_f1(model, target_testloader, criterion, device='cpu')\n",
    "            target_f1s.append(target_model_on_target_val_f1)\n",
    "\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(f'{experiments_dir}/{experiment}/{run}/best_base_model.pt', map_location='cpu'))\n",
    "            _,base_model_on_target_val_f1 = compute_loss_and_f1(model, target_testloader, criterion, device='cpu')\n",
    "            base_f1s.append(base_model_on_target_val_f1)\n",
    "\n",
    "            model.load_state_dict(torch.load(f'{experiments_dir}/{experiment}/{run}/best_target_model.pt', map_location='cpu'))\n",
    "            _,target_model_on_target_val_f1 = compute_loss_and_f1(model, target_testloader, criterion, device='cpu')\n",
    "\n",
    "            target_f1s.append(target_model_on_target_val_f1)\n",
    "\n",
    "        folds.append(run)\n",
    "\n",
    "    performances[experiment] = (base_f1s, target_f1s, folds)\n",
    "\n",
    "data = []\n",
    "for experiment, (base_f1s, target_f1s, folds) in performances.items():\n",
    "    for base_f1, target_f1, fold in zip(base_f1s, target_f1s, folds):\n",
    "        data.append({'experiment': experiment, 'model': 'base', 'f1': base_f1, 'fold': fold})\n",
    "        data.append({'experiment': experiment, 'model': 'target', 'f1': target_f1, 'fold': fold})\n",
    "\n",
    "df_original_testing = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f898ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original_testing.copy()\n",
    "df.dropna(inplace=True)\n",
    "df = df[~(df['model'] == 'base')]\n",
    "df.loc[df['experiment'].str.contains('full'),'model'] = 'full fine tuning'\n",
    "df.loc[df['experiment'].str.contains('target'),'model'] = 'target only'\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "df\n",
    "\n",
    "sns.lineplot(x='target training data percentage', y='f1', hue='model', marker='o', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original_testing.copy()\n",
    "df = df[df['experiment'].str.contains('full_fine_tuning')]\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df93bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x='fold', y='f1', hue='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd957444",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='target training data percentage', y='f1', hue='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = df_original_testing.copy()\n",
    "df = df[df['experiment'].str.contains('full_fine_tuning')]\n",
    "df.dropna(inplace=True)\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "\n",
    "sns.boxplot(data=df, x='target training data percentage', y='f1', hue='model')\n",
    "plt.title('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1cbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = df_original_testing.copy()\n",
    "df = df[df['experiment'].str.contains('full_fine_tuning')]\n",
    "df.dropna(inplace=True)\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "\n",
    "sns.boxplot(data=df, x='fold', y='f1', hue='model')\n",
    "plt.title('F1 Score vs Fold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1007316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original_testing.copy()\n",
    "df = df[df['experiment'].str.contains('full_fine_tuning_pct0.05')]\n",
    "df.dropna(inplace=True)\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "df\n",
    "# sns.lineplot(data=df, x='target training data percentage', y='f1', hue='fold')\n",
    "# plt.title('F1 Score vs Target Training Data Percentage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cbea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df[df['fold'].str.contains('fold2')]\n",
    "df = df[~(df['model'] == 'base')]\n",
    "df.loc[df['experiment'].str.contains('full'),'model'] = 'full fine tuning'\n",
    "df.loc[df['experiment'].str.contains('target'),'model'] = 'target only'\n",
    "\n",
    "df\n",
    "\n",
    "sns.lineplot(x='target training data percentage', y='f1', hue='model', marker='o', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11885a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_original_testing.copy()\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['fold'].str.contains('fold2')]\n",
    "df = df[~(df['model'] == 'base')]\n",
    "df.loc[df['experiment'].str.contains('full'),'model'] = 'full fine tuning'\n",
    "df.loc[df['experiment'].str.contains('target'),'model'] = 'target only'\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "df\n",
    "\n",
    "sns.lineplot(x='target training data percentage', y='f1', hue='model', marker='o', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using Testing Data Instead Of Validation\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from lib.utils_simple import evaluate, compute_loss_and_f1\n",
    "from lib.models import ConvLayerNorm, Block\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TestModel, self).__init__()\n",
    "        self.blocks = []\n",
    "        self.blocks.append(Block(6,8))\n",
    "        for _ in range(5):\n",
    "            self.blocks.append(Block(8,8))\n",
    "            self.blocks.append(Block(8,8,pool=False))\n",
    "\n",
    "        self.blocks.append(Block(8,16,pool=False))\n",
    "\n",
    "        # for _ in range(5):\n",
    "        #     self.blocks.append(Block(16,16))\n",
    "        #     self.blocks.append(Block(16,16,pool=False))\n",
    "            \n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.gap(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# Load all experiments into a dataframe\n",
    "performances = {}\n",
    "from tqdm import tqdm\n",
    "\n",
    "experiments_dir = f'./experiments_3_participants'\n",
    "\n",
    "for experiment in tqdm(os.listdir(experiments_dir)):\n",
    "    base_f1s = []\n",
    "    target_f1s = []\n",
    "    folds = []\n",
    "\n",
    "    for run in os.listdir(f'{experiments_dir}/{experiment}'):\n",
    "        if not os.path.exists(f'{experiments_dir}/{experiment}/{run}/metrics.json'):\n",
    "            continue\n",
    "        metrics = json.load(open(f'{experiments_dir}/{experiment}/{run}/metrics.json'))\n",
    "        losses = json.load(open(f'{experiments_dir}/{experiment}/{run}/losses.json'))\n",
    "        hyperparameters = json.load(open(f'{experiments_dir}/{experiment}/{run}/hyperparameters.json'))\n",
    "\n",
    "        data_path = hyperparameters['data_path']\n",
    "        target_participant = hyperparameters['target_participant']\n",
    "        target_testloader = DataLoader(TensorDataset(*torch.load(f'{data_path}/{target_participant}_test.pt')), batch_size=128, shuffle=False)\n",
    "\n",
    "        model = TestModel()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if 'target_only' in experiment:\n",
    "            base_model_on_target_val_f1 = None\n",
    "            base_f1s.append(base_model_on_target_val_f1)\n",
    "            model.load_state_dict(torch.load(f'{experiments_dir}/{experiment}/{run}/best_base_model.pt', map_location='cpu')) # naming is an artifact of training mode\n",
    "            _,target_model_on_target_val_f1 = compute_loss_and_f1(model, target_testloader, criterion, device='cpu')\n",
    "            target_f1s.append(target_model_on_target_val_f1)\n",
    "\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(f'{experiments_dir}/{experiment}/{run}/best_base_model.pt', map_location='cpu'))\n",
    "            _,base_model_on_target_val_f1 = compute_loss_and_f1(model, target_testloader, criterion, device='cpu')\n",
    "            base_f1s.append(base_model_on_target_val_f1)\n",
    "\n",
    "            model.load_state_dict(torch.load(f'{experiments_dir}/{experiment}/{run}/best_target_model.pt', map_location='cpu'))\n",
    "            _,target_model_on_target_val_f1 = compute_loss_and_f1(model, target_testloader, criterion, device='cpu')\n",
    "\n",
    "            target_f1s.append(target_model_on_target_val_f1)\n",
    "\n",
    "        folds.append(run)\n",
    "\n",
    "    performances[experiment] = (base_f1s, target_f1s, folds)\n",
    "\n",
    "data = []\n",
    "for experiment, (base_f1s, target_f1s, folds) in performances.items():\n",
    "    for base_f1, target_f1, fold in zip(base_f1s, target_f1s, folds):\n",
    "        data.append({'experiment': experiment, 'model': 'base', 'f1': base_f1, 'fold': fold})\n",
    "        data.append({'experiment': experiment, 'model': 'target', 'f1': target_f1, 'fold': fold})\n",
    "\n",
    "df_original = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6460f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "performances = {}\n",
    "for experiment in os.listdir('./experiments'):\n",
    "    base_f1s = []\n",
    "    target_f1s = []\n",
    "    folds = []\n",
    "    for run in os.listdir(f'./experiments/{experiment}'):\n",
    "        if not os.path.exists(f'./experiments/{experiment}/{run}/metrics.json'):\n",
    "            continue\n",
    "        metrics = json.load(open(f'./experiments/{experiment}/{run}/metrics.json'))\n",
    "        losses = json.load(open(f'./experiments/{experiment}/{run}/losses.json'))\n",
    "\n",
    "        if 'target_only' in experiment:\n",
    "            target_model_on_target_val_f1 = losses['target val f1'][metrics['best_base_val_loss_epoch']]\n",
    "            base_model_on_target_val_f1 = None\n",
    "        else:\n",
    "            target_model_on_target_val_f1 = losses['target val f1'][metrics['best_target_val_loss_epoch']]\n",
    "            base_model_on_target_val_f1 = losses['target val f1'][metrics['best_base_val_loss_epoch']]\n",
    "\n",
    "        base_f1s.append(base_model_on_target_val_f1)\n",
    "        target_f1s.append(target_model_on_target_val_f1)\n",
    "        folds.append(run)\n",
    "    performances[experiment] = (base_f1s, target_f1s, folds)\n",
    "\n",
    "# boxplot all performances\n",
    "import pandas as pd\n",
    "data = []\n",
    "for experiment, (base_f1s, target_f1s, folds) in performances.items():\n",
    "    for base_f1, target_f1, fold in zip(base_f1s, target_f1s, folds):\n",
    "        data.append({'experiment': experiment, 'model': 'base', 'f1': base_f1, 'fold': fold})\n",
    "        data.append({'experiment': experiment, 'model': 'target', 'f1': target_f1, 'fold': fold})\n",
    "df_original = pd.DataFrame(data)\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98b6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df_original.copy()\n",
    "df.dropna(inplace=True)\n",
    "# df = df[df['fold'].str.contains('fold2')]\n",
    "df = df[~(df['model'] == 'base')]\n",
    "df.loc[df['experiment'].str.contains('full'),'model'] = 'full fine tuning'\n",
    "df.loc[df['experiment'].str.contains('target'),'model'] = 'target only'\n",
    "df['experiment'] = df['experiment'].str.split('_').str[-3].str.replace('pct','').astype(float)\n",
    "df.rename(columns={'experiment':'target training data percentage'}, inplace=True)\n",
    "df\n",
    "\n",
    "sns.lineplot(x='target training data percentage', y='f1', hue='model', marker='o', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de475860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "for each full fine tuning experiment,\n",
    "\"\"\"\n",
    "performances = {}\n",
    "for experiment in os.listdir('./experiments'):\n",
    "    if 'full_fine_tuning' not in experiment:\n",
    "        continue\n",
    "    \n",
    "    base_f1s = []\n",
    "    target_f1s = []\n",
    "    folds = []\n",
    "    for run in os.listdir(f'./experiments/{experiment}'):\n",
    "        if not os.path.exists(f'./experiments/{experiment}/{run}/metrics.json'):\n",
    "            continue\n",
    "        metrics = json.load(open(f'./experiments/{experiment}/{run}/metrics.json'))\n",
    "        losses = json.load(open(f'./experiments/{experiment}/{run}/losses.json'))\n",
    "\n",
    "        if 'target_only' in experiment:\n",
    "            target_model_on_target_val_f1 = losses['target val f1'][metrics['best_base_val_loss_epoch']]\n",
    "            base_model_on_target_val_f1 = None\n",
    "        else:\n",
    "            target_model_on_target_val_f1 = losses['target val f1'][metrics['best_target_val_loss_epoch']]\n",
    "            base_model_on_target_val_f1 = losses['target val f1'][metrics['best_base_val_loss_epoch']]\n",
    "\n",
    "        base_f1s.append(base_model_on_target_val_f1)\n",
    "        target_f1s.append(target_model_on_target_val_f1)\n",
    "        folds.append(run)\n",
    "    performances[experiment] = (base_f1s, target_f1s, folds)\n",
    "\n",
    "# boxplot all performances\n",
    "import pandas as pd\n",
    "data = []\n",
    "for experiment, (base_f1s, target_f1s, folds) in performances.items():\n",
    "    for base_f1, target_f1, fold in zip(base_f1s, target_f1s, folds):\n",
    "        data.append({'experiment': experiment, 'model': 'base', 'f1': base_f1, 'fold': fold})\n",
    "        data.append({'experiment': experiment, 'model': 'target', 'f1': target_f1, 'fold': fold})\n",
    "df_original = pd.DataFrame(data)\n",
    "\n",
    "import seaborn as sns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
